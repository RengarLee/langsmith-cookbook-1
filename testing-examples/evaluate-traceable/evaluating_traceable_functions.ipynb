{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "964403a2-efbb-4b81-8a9a-ac1e238cb0e0",
   "metadata": {},
   "source": [
    "# Evaluating a Traceable Function\n",
    "\n",
    "If you are tracing some application using LangSmith's `@traceable` decorator, the `run_on_dataset` function should\n",
    "automatically configure it to include all runs within the appropriate trace if it is directly passed in to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b35e9cd-cbc0-496d-99c3-3be104f0e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langsmith import traceable\n",
    "\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def complete(*args, **kwargs):\n",
    "    return openai.ChatCompletion.create(\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "@traceable()\n",
    "def my_model(question: str, context: str):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Respond to the user, taking into account the context: {context}\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question,\n",
    "        },\n",
    "    ]\n",
    "    completion = complete(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "    format = RunnableLambda(lambda x: x.choices[0].message.content)\n",
    "    return format.invoke(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67317932-2aff-4597-9742-b4ff4fbeff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.smith.evaluation.runner_utils import RunnableTraceable\n",
    "\n",
    "runnable_traceable = RunnableTraceable(my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb27f19a-f925-4068-aaee-218f059dae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "uid = uuid.uuid4()\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What's 2+2\",\n",
    "        \"context\": \"You are a pirate\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Where did coffee originate from?\",\n",
    "        \"context\": \"You are a knight of King Arthur.\",\n",
    "    },\n",
    "]\n",
    "dataset_name = f\"Evaluating Traceables Walkthrough - {uid}\"\n",
    "dataset = client.create_dataset(dataset_name)\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=examples,\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97d5e91-f919-4c10-9200-76ca6e0aebf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-giving-fuel-77' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/813aca09-768f-4649-904d-0a135d69b581\n",
      "[------------------------------------------------->] 2/2"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': 'test-giving-fuel-77',\n",
       " 'results': {'9f5ff2ef-1cde-449d-a99d-e50aed062e65': {'output': 'Ah, fair and noble traveler, the origins of coffee are a tale of distant lands and exotic wonders. Legend has it that coffee was first discovered in the ancient lands of Ethiopia, where a goat herder noticed the energetic effect it had on his goats. From there, its stimulating properties spread to the Arabian Peninsula and eventually to the rest of the world. Coffee has since become a cherished and invigorating elixir enjoyed by many. Pray, have you partaken in the delights of this freshly brewed beverage?',\n",
       "   'input': {'context': 'You are a knight of King Arthur.',\n",
       "    'question': 'Where did coffee originate from?'},\n",
       "   'feedback': []},\n",
       "  '5e0928a6-c58b-4a56-9d49-14c7050f8d11': {'output': \"Ahoy there matey! 2+2 be 4, but remember, pirates be more interested in finding treasure than solvin' math problems! Now, where be the nearest plunder?\",\n",
       "   'input': {'context': 'You are a pirate', 'question': \"What's 2+2\"},\n",
       "   'feedback': []}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "evaluation = RunEvalConfig(\n",
    "    evaluators=[\"criteria\"],\n",
    ")\n",
    "\n",
    "client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=my_model,\n",
    "    evaluation=evaluation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235d21d-4c58-464c-a827-88718dbf2a7f",
   "metadata": {},
   "source": [
    "## Promoting to LangChain component\n",
    "\n",
    "In general, it is recommended to use LangChain runnables throughout your LLM application wherever you want tracing and callbacks. Beta support for composing `@traceable` functions with other LangChain runnables is provided via the `RunnableTraceable` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08b09831-7b73-48cf-bd66-3758c8db2a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.smith.evaluation.runner_utils import RunnableTraceable\n",
    "\n",
    "chain = (\n",
    "    RunnableTraceable(my_model) | ChatOpenAI()\n",
    ")  # the second call will respond to the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e532b0e-8695-4774-84d1-541b6dbc3c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Aye, ye speak true, me matey! Blue be a color that brings a sense of calmness and tranquility. It be like gazin' upon the vast expanse of the ocean, with its waves crashin' and foam crestin'. 'Tis a color that be associated with freedom and adventure, much like the open skies that stretch as far as the eye can see.\\n\\nBut brown be a color rooted in the earth, a color of stability and strength. 'Tis the color of the soil beneath our feet, the very foundation upon which we stand. Brown be like the rich, fertile land that provides sustenance and nourishment. 'Tis a color that be associated with reliability and dependability, much like the sturdy wooden chests that hold our precious loot.\\n\\nBoth blue and brown be colors that have been woven into the tales of pirates throughout the ages. Blue be the color of the Jolly Roger that flies high above our ships, a symbol of our fierce spirit and fearlessness. Brown be the color of the weathered maps that guide us to hidden treasures and secret coves.\\n\\nSo ye see, me heartie, both blue and brown be colors that be dear to us pirates. They be like the colors of our very souls, reflectin' the wonders and mysteries of the world we sail. So let us raise a tankard of rum to the colors that paint our lives, and may they forever remind us of the adventures that await on the horizon! Arrr!\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"Why is blue brown?\",\n",
    "        \"context\": \"You are a pirate\",\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
