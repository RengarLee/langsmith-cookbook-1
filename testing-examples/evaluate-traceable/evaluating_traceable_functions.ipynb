{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "964403a2-efbb-4b81-8a9a-ac1e238cb0e0",
   "metadata": {},
   "source": [
    "# Evaluating a Traceable Function\n",
    "\n",
    "If you are tracing some application using LangSmith's `@traceable` decorator, the `run_on_dataset` function should\n",
    "automatically configure it to include all runs within the appropriate trace if it is directly passed in to the function.\n",
    "\n",
    "This functionality is supported in `langchain>=0.0.323`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b35e9cd-cbc0-496d-99c3-3be104f0e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langsmith import traceable\n",
    "\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def complete(*args, **kwargs):\n",
    "    return openai.ChatCompletion.create(\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "@traceable()\n",
    "def my_model(question: str, context: str):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Respond to the user, taking into account the context: {context}\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question,\n",
    "        },\n",
    "    ]\n",
    "    completion = complete(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "    # It should still work if a LangChain component is called within a traceable function\n",
    "    format = RunnableLambda(lambda x: x.choices[0].message.content)\n",
    "    return format.invoke(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb27f19a-f925-4068-aaee-218f059dae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "uid = uuid.uuid4()\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What's 2+2\",\n",
    "        \"context\": \"You are a pirate\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Where did coffee originate from?\",\n",
    "        \"context\": \"You are a knight of King Arthur.\",\n",
    "    },\n",
    "]\n",
    "dataset_name = f\"Evaluating Traceables Walkthrough - {uid}\"\n",
    "dataset = client.create_dataset(dataset_name)\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=examples,\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a97d5e91-f919-4c10-9200-76ca6e0aebf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-abandoned-blossom-92' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/e11bc750-2e63-4109-907f-61dd905588fa?eval=true\n",
      "[------------------------------------------------->] 2/2"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': 'test-abandoned-blossom-92',\n",
       " 'results': {'03ae9ac9-161a-4eea-9446-a7ca5562b96f': {'output': 'My liege, coffee is said to have originated in the ancient land of Ethiopia. Legend has it that a goat herder named Kaldi discovered the energizing effects of coffee beans after noticing his goats became lively and energetic upon consuming them. This marvelous drink eventually made its way to the Arabian Peninsula and beyond, becoming a cherished beverage throughout the world.',\n",
       "   'input': {'context': 'You are a knight of King Arthur.',\n",
       "    'question': 'Where did coffee originate from?'},\n",
       "   'feedback': []},\n",
       "  '508cf84a-af98-4283-bcd5-e2bf59801f99': {'output': \"Ahoy, matey! The answer be 4! If ye be lookin' for a challenge, I can teach ye the ways of navigation or the art of swashbucklin' instead!\",\n",
       "   'input': {'context': 'You are a pirate', 'question': \"What's 2+2\"},\n",
       "   'feedback': []}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "evaluation = RunEvalConfig(\n",
    "    evaluators=[\"criteria\"],\n",
    ")\n",
    "\n",
    "client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=my_model,\n",
    "    evaluation=evaluation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235d21d-4c58-464c-a827-88718dbf2a7f",
   "metadata": {},
   "source": [
    "## Promoting to LangChain component\n",
    "\n",
    "In general, it is recommended to use LangChain runnables throughout your LLM application wherever you want tracing and callbacks. Beta support for composing `@traceable` functions with other LangChain runnables is provided via the `RunnableTraceable` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08b09831-7b73-48cf-bd66-3758c8db2a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langsmith.run_helpers import as_runnable\n",
    "\n",
    "chain = (\n",
    "    as_runnable(my_model) | ChatOpenAI()\n",
    ")  # the second call will respond to the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e532b0e-8695-4774-84d1-541b6dbc3c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Ahoy there, me hearty! Ye be askin' a question about the colors of the sea, aye? Well, let me tell ye a tale. The sea be a mighty and mysterious place, filled with wonders beyond our reckonin'. It be said that the color of the sea be determined by many factors, such as the depth, the presence of certain organisms, and even the weather. \\n\\nWhen the sun be high in the sky, the sea often appears a shimmerin' shade of blue. This be because the water molecules be absorbin' most of the colors in the sunlight, 'cept for the blue wavelengths. So, what ye be seein' be the reflected blue light dancin' upon the surface of the water.\\n\\nBut if ye be sailin' into deeper waters, ye might find the sea takin' on a darker hue, like a rich navy blue or even black. This be because as ye descend into the depths, less light be able to penetrate the water, leavin' only the deepest blues to be seen.\\n\\nNow, there be times when the sea be not blue at all, but a mighty shade of brown. This be known as a brown tide, me matey. It be caused by a bloom of certain microscopic organisms, such as algae or dinoflagellates. These critters be producin' pigments that turn the water brown, makin' it a sight to behold.\\n\\nSo, ye see, the colors of the sea be a marvel of nature, influenced by the sun, the depth, and the creatures that call the ocean home. No hidden treasures or magical tricks, but still a wondrous sight to behold. Now, go forth and set sail, me matey, and may ye find many a colorful sea on yer adventures! Arr!\")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"Why is blue brown?\",\n",
    "        \"context\": \"You are a pirate\",\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
