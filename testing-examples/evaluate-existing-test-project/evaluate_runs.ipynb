{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddb1a3b-eaf7-4755-8bfe-4d9178c7927a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluating Existing Runs\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langsmith-cookbook/blob/main/testing-examples/evaluate-existing-test-project/evaluate_runs.ipynb)\n",
    "\n",
    "This tutorial shows how to evaluate runs in an existing test project. This is useful when:\n",
    "- You have a new evaluator or version of an evaluator and want to add the eval metrics to existing test projects\n",
    "- Your model isn't defined in python or typescript but you want to add evaluation metrics\n",
    "\n",
    "The steps are:\n",
    "\n",
    "1. Select the test project you wish to evaluate \n",
    "2. Define the RunEvaluator\n",
    "3. Call the `client.evaluate_run` method, which runs the evaluation and logs the results as feedback.\n",
    "\n",
    "    - _alternatively, call `client.create_feedback` method directly, since evaluation results are logged as model feedback_\n",
    "    \n",
    "\n",
    "This is all you need to start logging eval feedback to an existing project.\n",
    "Below, we will review how to list the runs to evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2dbea3-77d3-4d2a-bc6e-39e3688eadab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Select Test Project to evaluate\n",
    "\n",
    "Each time you  call `run_on_dataset` to evaluate a model, a new \"test project\" is created containing the model's runs and the evaluator feedback. Each run contains the inputs and outputs to the component as well as a reference to the dataset example (row) it came from. The test project URL and name is printed to stdout when the function is called.\n",
    "\n",
    "The easiest way to find the test project name or ID is in the web app. Navigate to \"Datasets & Testing\", select a dataset, and then copy one of the project names from the test runs table. Below is an example of the Dataset & Testing page, with all the datasets listed out. We will select the \"Chat Langchain Questions\" dataset.\n",
    "\n",
    "![Datasets & Testing Page](./img/datasets_and_testing.png)\n",
    "\n",
    "Once you've selected one of the datasets, a list of test projects will be displayed. You can copy the project name from the table directly.\n",
    "\n",
    "![Test Projects](./img/test_projects_page.png)\n",
    "\n",
    "Or if you navigate to the test page, you can copy the project name from the title or the ID from the url.\n",
    "\n",
    "![Test Page](./img/test_page.png)\n",
    "\n",
    "\n",
    "Then once you have the project name or ID, you can list the runs to evaluate by calling `list_runs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c17fb13f934c7bbc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\" # Update with your API key\n",
    "project_name = \"YOUR PROJECT NAME\" # Update with your project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21249b59-ed95-49b3-b06d-bdecc6159bb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Copy the project name or ID and paste it in the corresponding field below\n",
    "runs = client.list_runs(\n",
    "    project_name = project_name,\n",
    "    # Or by ID\n",
    "    # project_id = \"0fc4f999-bdd3-4a7e-b2d7-bdf837d57cd9\",\n",
    "    execution_order = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83baa90c-27ee-4539-8774-44191bab7ba9",
   "metadata": {},
   "source": [
    "Since this is a test project, each run will have a reference to the dataset example, meaning you can apply a labeled evaluator such as the [cot_qa](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.qa.eval_chain.CotQAEvalChain.html#langchain.evaluation.qa.eval_chain.CotQAEvalChain) evaluator to these runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48efb84a-c84a-4903-95a8-e6c0d1e47fa4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Define Evaluator\n",
    "\n",
    "You may already know what you want to test to ensure you application is functioning as expected. In that case, you can easily add that logic to a custom evaluator to get started.\n",
    "You can also configure one of LangChain's off-the-shelf evaluators to use to test for things like correctness, helpfulness, embedding or string distance, or other metrics.\n",
    "For more information on some of the existing open source evaluators, check out the [documentation](https://python.langchain.com/docs/guides/evaluation).\n",
    "\n",
    "### Custom Evaluator\n",
    "\n",
    "You can add automated/algorithmic feedback to existing runs using just the SDK in two steps:\n",
    "1. Subclassing the RunEvaluator class and implementing the evaluate_run method\n",
    "2. Calling the `evaluate_run` method directly on the client\n",
    "\n",
    "The `evaluate_run` method loads a reference example if present, applies the evaluator to the run and optional example, and then logs the feedback to LangSmith.\n",
    "Below, create a custom evaluator that checks for any digits in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6f9459-51fa-468c-bc65-0b965f5ba628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langsmith.evaluation import EvaluationResult, RunEvaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "\n",
    "class ContainsDigits(RunEvaluator):\n",
    "\n",
    "    def evaluate_run(\n",
    "        self, run: Run, example: Optional[Example] = None\n",
    "    ) -> EvaluationResult:\n",
    "        if run.outputs is None:\n",
    "            raise ValueError(\"Run outputs cannot be None\")\n",
    "        prediction = str(next(iter(run.outputs.values())))\n",
    "        contains_digits = any(c.isdigit() for c in prediction)\n",
    "        print(f\"Evaluating run:  {run.id}\")\n",
    "        return EvaluationResult(key=\"Contains Digits\", score=contains_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb96fbe-ff1b-42d1-be98-ce4d2f312b39",
   "metadata": {},
   "source": [
    "Our custom evaluator is a simple reference-free check for boolean presence of digits in the output. In your case you may want to check for PII, assert the result conforms to some schema, or even parse and compare generated code.\n",
    "\n",
    "The logic fetching the prediction above assumes your chain only returns one value, meaning the `run.outputs` dictionary will have only one key. If there are multiple keys in your outputs, you will have to select whichever key(s) you wish to evaluate or test the whole outputs dictionary directly as a string. For more information on creating a custom evaluator, check out the [docs](https://docs.smith.langchain.com/evaluation/custom-evaluators).\n",
    "\n",
    "Below, apply the evaluator to all runs in the \"My Test\" project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df31b712-232e-4bfc-8298-4d6b656a02b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluting run:  bdd21374-8b04-4af1-b052-cd780274d8a4\n",
      "Evaluting run:  77b6121e-3f5e-4e4c-a054-dfd2f51a2f8b\n",
      "Evaluting run:  f3f5eee4-6e9e-494b-b975-13edbcd79730\n",
      "Evaluting run:  62ffea84-49ff-464f-8a65-fa4ecfd3c02e\n",
      "Evaluting run:  b33c3adc-aa51-4259-a0b0-1f8b52c8c135\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "evaluator = ContainsDigits()\n",
    "runs = client.list_runs(\n",
    "    project_name=project_name,\n",
    "    execution_order=1,\n",
    "    error=False,\n",
    ")\n",
    "\n",
    "for run in itertools.islice(runs, 5):\n",
    "    feedback = client.evaluate_run(run, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf14eb40-dd2d-416b-889a-ef879c21a1b3",
   "metadata": {},
   "source": [
    "The evaluation results will all be saved as feedback to the run trace. LangSmith aggregates the feedback over the project for you asynchronously, so after some time you will be\n",
    "able to see the feedback results directly on the project stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "088e262f-6f82-4e0b-afee-e414cc9bd38b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smog_index': {'n': 198, 'avg': 0.0, 'mode': 0, 'is_all_model': True},\n",
       " 'user_click': {'n': 13, 'avg': 1.0, 'mode': 1, 'is_all_model': False},\n",
       " 'completeness': {'n': 275,\n",
       "  'avg': 0.851063829787234,\n",
       "  'mode': 1,\n",
       "  'is_all_model': True},\n",
       " 'user_feedback': {'n': 6, 'avg': 0.75, 'mode': 0.5, 'is_all_model': False},\n",
       " 'Contains Digits': {'n': 945,\n",
       "  'avg': 0.638095238095238,\n",
       "  'mode': 1,\n",
       "  'is_all_model': True},\n",
       " 'sufficient_code': {'n': 220, 'avg': 1.0, 'mode': 1, 'is_all_model': True},\n",
       " 'coleman_liau_index': {'n': 198,\n",
       "  'avg': -0.30404040404040406,\n",
       "  'mode': 11.15,\n",
       "  'is_all_model': True},\n",
       " 'flesch_reading_ease': {'n': 198,\n",
       "  'avg': 82.80222222222223,\n",
       "  'mode': 59.97,\n",
       "  'is_all_model': True},\n",
       " 'flesch_kincaid_grade': {'n': 198,\n",
       "  'avg': 2.436868686868687,\n",
       "  'mode': 5.6,\n",
       "  'is_all_model': True},\n",
       " 'automated_readability_index': {'n': 198,\n",
       "  'avg': 4.785858585858586,\n",
       "  'mode': 9.9,\n",
       "  'is_all_model': True}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Updating the aggregate stats is async, but after some time, the \"Contains Digits\" feedback will be available\n",
    "client.read_project(project_name=project_name).feedback_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f59c106-7a98-41ef-a769-78005b6ab6c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LangChain evaluators\n",
    "\n",
    "LangChain has a number of evaluators you can  use off-the-shelf or modify to suit your needs. An easy way to use these is to modify the code above and apply the evaluator directly to the run. For more information on available LangChain evaluators, check out the [open source documentation](https://python.langchain.com/docs/guides/evaluation).\n",
    "\n",
    "Below, we will demonstrate this by using the criteria evaluator, which instructs an LLM to check that the prediction against the described criteria. \n",
    "In this case, we will check that the responses contain both a python and typescript example, if needed, since LangSmith's SDK supports both languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb7a8e56-ad86-4e7e-809b-f86704f0cd2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import evaluation, callbacks\n",
    "\n",
    "class SufficientCodeEvaluator(RunEvaluator):\n",
    "    \n",
    "    def __init__(self):\n",
    "        criteria_description=(\n",
    "            \"If the submission contains code, does it contain both a python and typescript example?\"\n",
    "            \" Y if no code is needed or if both languages are present, N if response is only in one language\"\n",
    "        )\n",
    "        self.evaluator = evaluation.load_evaluator(\"criteria\", \n",
    "                                      criteria={\n",
    "                                          \"sufficient_code\": criteria_description\n",
    "                                      })\n",
    "    def evaluate_run(\n",
    "        self, run: Run, example: Optional[Example] = None\n",
    "    ) -> EvaluationResult:\n",
    "        question = next(iter(run.inputs.values()))\n",
    "        prediction = str(next(iter(run.outputs.values())))\n",
    "        print(f\"Evaluating run:  {run.id}\")\n",
    "        with callbacks.collect_runs() as cb:\n",
    "            result = self.evaluator.evaluate_strings(input=question, prediction=prediction)\n",
    "            run_id = cb.traced_runs[0].id\n",
    "        return EvaluationResult(key=\"sufficient_code\", evaluator_info={\"__run\": {\"run_id\": run_id}}, **result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9538ba5a-3b5b-47b0-a082-1a27b7cda014",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating run:  bdd21374-8b04-4af1-b052-cd780274d8a4\n",
      "Evaluating run:  77b6121e-3f5e-4e4c-a054-dfd2f51a2f8b\n",
      "Evaluating run:  f3f5eee4-6e9e-494b-b975-13edbcd79730\n",
      "Evaluating run:  62ffea84-49ff-464f-8a65-fa4ecfd3c02e\n",
      "Evaluating run:  b33c3adc-aa51-4259-a0b0-1f8b52c8c135\n"
     ]
    }
   ],
   "source": [
    "runs = client.list_runs(\n",
    "    project_name=project_name,\n",
    "    execution_order=1,\n",
    "    error=False,\n",
    ")\n",
    "evaluator = SufficientCodeEvaluator()\n",
    "for run in itertools.islice(runs, 5):\n",
    "    feedback = client.evaluate_run(run, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb41ef-3892-4385-8830-c6decfbf8f5c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congrats! You've run evals on an existing test project and logged feedback to the traces. Now, all the feedback results are aggregated on the project page, and you can use those to compare prompts and chains on a dataset.\n",
    "\n",
    "If you have other related questions, feel free to create an issue in this repo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
