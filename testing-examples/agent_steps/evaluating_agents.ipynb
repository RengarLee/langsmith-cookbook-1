{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f14d898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da61d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f7730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get a weather report for a given city.\"\"\"\n",
    "    return \"52 degrees F. Scattered showers throughout the day, becoming more strong at 11:35am.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef814332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25137573",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful assistant.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b455fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9427291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo-16k\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    DuckDuckGoSearchResults(\n",
    "        name=\"duck_duck_go\"\n",
    "    ),  # General internet search using DuckDuckGo\n",
    "]\n",
    "\n",
    "llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools])\n",
    "\n",
    "runnable_agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_functions(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=runnable_agent, tools=tools, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e60b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"What is LangSmith? Use duck duck go to look it up\",\n",
    "    \"Hi\",\n",
    "]\n",
    "\n",
    "results = agent_executor.batch([{\"input\": x} for x in inputs], return_exceptions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ed21ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [\n",
    "    \"LangSmith is a unified platform for debugging, testing, and monitoring language model applications and agents powered by LangChain\",\n",
    "    \"Hi\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "046b2402",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = f\"agent-qa-0124\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name, description=\"An example dataset of questions over the LangSmith documentation.\"\n",
    ")\n",
    "\n",
    "for query, answer in zip(inputs, outputs):\n",
    "    client.create_example(inputs={\"input\": query}, outputs={\"output\": answer}, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64e987d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools, AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "# Since chains can be stateful (e.g. they can have memory), we provide\n",
    "# a way to initialize a new chain for each row in the dataset. This is done\n",
    "# by passing in a factory function that returns a new chain for each row.\n",
    "def agent_factory():    \n",
    "    llm_with_tools = llm.bind(\n",
    "        functions=[format_tool_to_openai_function(t) for t in tools]\n",
    "    )\n",
    "    runnable_agent = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"agent_scratchpad\": lambda x: format_to_openai_functions(x['intermediate_steps'])\n",
    "            } \n",
    "             | prompt \n",
    "             | llm_with_tools \n",
    "             | OpenAIFunctionsAgentOutputParser()\n",
    "    )\n",
    "    return  AgentExecutor(agent=runnable_agent, tools=tools, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55659b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    # Evaluators can either be an evaluator type (e.g., \"qa\", \"criteria\", \"embedding_distance\", etc.) or a configuration for that evaluator\n",
    "    evaluators=[\n",
    "        # Measures whether a QA response is \"Correct\", based on a reference answer\n",
    "        # You can also select via the raw string \"qa\"\n",
    "        EvaluatorType.QA,\n",
    "\n",
    "    ],\n",
    "    # You can add custom StringEvaluator or RunEvaluator objects here as well, which will automatically be\n",
    "    # applied to each prediction. Check out the docs for examples.\n",
    "    custom_evaluators=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa8ec349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'runnable-agent-test-5d466cbc-0124' at:\n",
      "https://dev.smith.langchain.com/o/40ec80b0-9ee0-4eb0-aa65-e97158d191c5/projects/p/6282437a-da57-4317-9e0b-1759b9ecc923\n",
      "[------------------------------------------------->] 2/2\n",
      " Eval quantiles:\n",
      "             0.25  0.5  0.75  mean  mode\n",
      "correctness   1.0  1.0   1.0   1.0   1.0\n"
     ]
    }
   ],
   "source": [
    "from langchain.smith import (\n",
    "    arun_on_dataset,\n",
    "    run_on_dataset, \n",
    ")\n",
    "\n",
    "chain_results = run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=agent_factory,\n",
    "    evaluation=evaluation_config,\n",
    "    verbose=True,\n",
    "    client=client,\n",
    "    project_name=f\"runnable-agent-test-5d466cbc-0124\",\n",
    "    tags=[\"testing-notebook\", \"prompt:5d466cbc\"],  # Optional, adds a tag to the resulting chain runs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9badeba5",
   "metadata": {},
   "source": [
    "# How to test intermediate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f526e117",
   "metadata": {},
   "source": [
    "Dataset prep: define a new dataset, now we are adding the expected intermediate steps to this dataset. Note that here you can add whatever you want to use in your evaluation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "129154f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = f\"agent-qa-intermediate-0124\"\n",
    "\n",
    "intermediate_steps = [\n",
    "    # First datapoint we expect to call duck_duck_go once\n",
    "    [\"duck_duck_go\"], \n",
    "    # The next datapoint we expect to call no tools\n",
    "    []\n",
    "]\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name, description=\"An example dataset of questions over the LangSmith documentation.\"\n",
    ")\n",
    "\n",
    "for query, answer, steps in zip(inputs, outputs, intermediate_steps):\n",
    "    client.create_example(inputs={\"input\": query}, outputs={\"output\": answer, \"steps\": steps}, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ad7d5",
   "metadata": {},
   "source": [
    "First: add in `return_intermediate_steps=True to the AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "baa875e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_factory():    \n",
    "    llm_with_tools = llm.bind(\n",
    "        functions=[format_tool_to_openai_function(t) for t in tools]\n",
    "    )\n",
    "    runnable_agent = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"agent_scratchpad\": lambda x: format_to_openai_functions(x['intermediate_steps'])\n",
    "            } \n",
    "             | prompt \n",
    "             | llm_with_tools \n",
    "             | OpenAIFunctionsAgentOutputParser()\n",
    "    )\n",
    "    \n",
    "    return  AgentExecutor(agent=runnable_agent, tools=tools, handle_parsing_errors=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464d318",
   "metadata": {},
   "source": [
    "Second: define a custom evaluator to evaluate the agent trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7c701d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langsmith.evaluation import EvaluationResult, RunEvaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "\n",
    "class AgentTrajectoryEvaluator(RunEvaluator):\n",
    "\n",
    "    def evaluate_run(\n",
    "        self, run: Run, example: Optional[Example] = None\n",
    "    ) -> EvaluationResult:\n",
    "        if run.outputs is None:\n",
    "            raise ValueError(\"Run outputs cannot be None\")\n",
    "        # This is the output of each run\n",
    "        intermediate_steps = run.outputs[\"intermediate_steps\"]\n",
    "        # Since we are comparing to the tool names, we now need to get that\n",
    "        # Intermediate steps is a Tuple[AgentAction, Any]\n",
    "        # The first element is the action taken\n",
    "        # The second element is the observation from taking that action\n",
    "        trajectory = [action.tool for action, _ in intermediate_steps]\n",
    "        # This is what we uploaded to the dataset\n",
    "        expected_trajectory = example.outputs['steps']\n",
    "        # Just score it based on whether it is correct or not\n",
    "        score = int(trajectory == expected_trajectory)\n",
    "        return EvaluationResult(key=\"Intermediate steps correctness\", score=score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8568f90",
   "metadata": {},
   "source": [
    "Third: add that evaluator to the `custom_evaluators` list\n",
    "\n",
    "Also, specify the reference key (since we have multiple outputs in our dataset now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5201d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    # Evaluators can either be an evaluator type (e.g., \"qa\", \"criteria\", \"embedding_distance\", etc.) or a configuration for that evaluator\n",
    "    evaluators=[\n",
    "        # Measures whether a QA response is \"Correct\", based on a reference answer\n",
    "        # You can also select via the raw string \"qa\"\n",
    "        EvaluatorType.QA,\n",
    "\n",
    "    ],\n",
    "    # You can add custom StringEvaluator or RunEvaluator objects here as well, which will automatically be\n",
    "    # applied to each prediction. Check out the docs for examples.\n",
    "    custom_evaluators=[AgentTrajectoryEvaluator()],\n",
    "    # We now need to specify this because we have multiple outputs in our dataset\n",
    "    reference_key=\"output\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e755b38b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'runnable-agent-test-5d466cbc-0130' at:\n",
      "https://dev.smith.langchain.com/o/40ec80b0-9ee0-4eb0-aa65-e97158d191c5/projects/p/c1cabd91-60d0-4f55-b8c5-498380eb81ef\n",
      "[------------------------>                         ] 1/2[]\n",
      "[------------------------------------------------->] 2/2['duck_duck_go']\n",
      "\n",
      " Eval quantiles:\n",
      "                                0.25  0.5  0.75  mean  mode\n",
      "Intermediate steps correctness  1.00  1.0  1.00   1.0   1.0\n",
      "correctness                     0.25  0.5  0.75   0.5   0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain.smith import (\n",
    "    arun_on_dataset,\n",
    "    run_on_dataset, \n",
    ")\n",
    "\n",
    "chain_results = run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=agent_factory,\n",
    "    evaluation=evaluation_config,\n",
    "    verbose=True,\n",
    "    client=client,\n",
    "    project_name=f\"runnable-agent-test-5d466cbc-0130\",\n",
    "    tags=[\"testing-notebook\", \"prompt:5d466cbc\"],  # Optional, adds a tag to the resulting chain runs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e70b867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9364934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c4359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
