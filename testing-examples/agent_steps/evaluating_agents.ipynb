{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f14d898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64e987d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools, AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "# Since chains can be stateful (e.g. they can have memory), we provide\n",
    "# a way to initialize a new chain for each row in the dataset. This is done\n",
    "# by passing in a factory function that returns a new chain for each row.\n",
    "def agent_factory():    \n",
    "    llm_with_tools = llm.bind(\n",
    "        functions=[format_tool_to_openai_function(t) for t in tools]\n",
    "    )\n",
    "    runnable_agent = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"agent_scratchpad\": lambda x: format_to_openai_functions(x['intermediate_steps'])\n",
    "            } \n",
    "             | prompt \n",
    "             | llm_with_tools \n",
    "             | OpenAIFunctionsAgentOutputParser()\n",
    "    )\n",
    "    return  AgentExecutor(agent=runnable_agent, tools=tools, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55659b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    # Evaluators can either be an evaluator type (e.g., \"qa\", \"criteria\", \"embedding_distance\", etc.) or a configuration for that evaluator\n",
    "    evaluators=[\n",
    "        # Measures whether a QA response is \"Correct\", based on a reference answer\n",
    "        # You can also select via the raw string \"qa\"\n",
    "        EvaluatorType.QA,\n",
    "\n",
    "    ],\n",
    "    # You can add custom StringEvaluator or RunEvaluator objects here as well, which will automatically be\n",
    "    # applied to each prediction. Check out the docs for examples.\n",
    "    custom_evaluators=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9badeba5",
   "metadata": {},
   "source": [
    "# How to test intermediate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f526e117",
   "metadata": {},
   "source": [
    "Dataset prep: define a new dataset, now we are adding the expected intermediate steps to this dataset. Note that here you can add whatever you want to use in your evaluation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "129154f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "questions = [\n",
    "    (\n",
    "        \"Why was was a $10 calculator app one of the best-rated Nintendo Switch games?\", \n",
    "        {\n",
    "            \"reference\": \"It became an internet meme due to its high price point.\",\n",
    "            \"expected_steps\": [\"duck_duck_go\"], \n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        \"hi\", \n",
    "        {\n",
    "            \"reference\": \"Hello, how can I assist you?\",\n",
    "            \"expected_steps\": [], # Expect a direct response\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        \"Who is Dejan Trajkov?\", \n",
    "         {\n",
    "             \"reference\": \"Macedonian Professor, Immunologist and Physician\",\n",
    "             \"expected_steps\": [\"duck_duck_go\"], \n",
    "         }\n",
    "    ),\n",
    "    (\n",
    "         \"Who won the 2023 U23 world wresting champs (men's freestyle 92 kg)\", \n",
    "         {\n",
    "             \"reference\": \"Muhammed Gimri from turkey\",\n",
    "             \"expected_steps\": [\"duck_duck_go\"], \n",
    "         }\n",
    "    ),\n",
    "    (\n",
    "        \"What's my first meeting on Friday?\", \n",
    "        {\n",
    "             \"reference\": 'Your first meeting is 8:30 AM for \"Team Standup\"',\n",
    "             \"expected_steps\": [\"check_calendar\"], # Only expect calendar tool\n",
    "         }\n",
    "    ),\n",
    "]\n",
    "\n",
    "uid = uuid.uuid4()\n",
    "dataset_name = f\"Agent Eval Example {uid}\"\n",
    "ds = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"An example agent evals dataset using search and calendar checks.\"\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q[0]} for q in questions],\n",
    "    outputs=[q[1] for q in questions],\n",
    "    dataset_id=ds.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "baa875e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.tools import tool\n",
    "from datetime import datetime\n",
    "\n",
    "@tool\n",
    "def check_calendar(date: datetime, run_manager = None) -> list:\n",
    "    \"\"\"Check the user's calendar for a meetings on the specified day.\"\"\"\n",
    "    # A placeholder to demonstrate with multiple tools.\n",
    "    # It's easy to mock tools when testing.\n",
    "    if date.weekday() == 4:\n",
    "        return [\n",
    "            \"8:30 : Team Standup\",\n",
    "            \"9:00 : 1 on 1\",\n",
    "            \"9:45 design review\",\n",
    "        ]\n",
    "    return [\"Focus time\"] # If only...\n",
    "    \n",
    "    \n",
    "\n",
    "def agent_factory():    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        temperature=0,\n",
    "    )\n",
    "    tools = [\n",
    "        DuckDuckGoSearchResults(\n",
    "            name=\"duck_duck_go\"\n",
    "        ),  # General internet search using DuckDuckGo\n",
    "        check_calendar,\n",
    "    ]        \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ])\n",
    "\n",
    "    runnable_agent = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"question\"],\n",
    "                \"agent_scratchpad\": lambda x: format_to_openai_functions(x['intermediate_steps'])\n",
    "            } \n",
    "             | prompt \n",
    "             | llm_with_tools \n",
    "             | OpenAIFunctionsAgentOutputParser()\n",
    "    )\n",
    "    \n",
    "    return AgentExecutor(\n",
    "        agent=runnable_agent, \n",
    "        tools=tools, \n",
    "        handle_parsing_errors=True, \n",
    "        return_intermediate_steps=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464d318",
   "metadata": {},
   "source": [
    "## 2. define a custom evaluator to evaluate the agent trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7c701d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langsmith.evaluation import EvaluationResult, RunEvaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "\n",
    "class AgentTrajectoryEvaluator(RunEvaluator):\n",
    "\n",
    "    def evaluate_run(\n",
    "        self, run: Run, example: Optional[Example] = None\n",
    "    ) -> EvaluationResult:\n",
    "        if run.outputs is None:\n",
    "            raise ValueError(\"Run outputs cannot be None\")\n",
    "        # This is the output of each run\n",
    "        intermediate_steps = run.outputs[\"intermediate_steps\"]\n",
    "        # Since we are comparing to the tool names, we now need to get that\n",
    "        # Intermediate steps is a Tuple[AgentAction, Any]\n",
    "        # The first element is the action taken\n",
    "        # The second element is the observation from taking that action\n",
    "        trajectory = [action.tool for action, _ in intermediate_steps]\n",
    "        # This is what we uploaded to the dataset\n",
    "        expected_trajectory = example.outputs['expected_steps']\n",
    "        # Just score it based on whether it is correct or not\n",
    "        score = int(trajectory == expected_trajectory)\n",
    "        return EvaluationResult(key=\"Intermediate steps correctness\", score=score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8568f90",
   "metadata": {},
   "source": [
    "Third: add that evaluator to the `custom_evaluators` list\n",
    "\n",
    "Also, specify the reference key (since we have multiple outputs in our dataset now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5201d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    # Evaluators can either be an evaluator type (e.g., \"qa\", \"criteria\", \"embedding_distance\", etc.) or a configuration for that evaluator\n",
    "    evaluators=[\n",
    "        # Measures whether a QA response is \"Correct\", based on a reference answer\n",
    "        # You can also select via the raw string \"qa\"\n",
    "        EvaluatorType.QA,\n",
    "\n",
    "    ],\n",
    "    # You can add custom StringEvaluator or RunEvaluator objects here as well, which will automatically be\n",
    "    # applied to each prediction. Check out the docs for examples.\n",
    "    custom_evaluators=[AgentTrajectoryEvaluator()],\n",
    "    # We now need to specify this because we have multiple outputs in our dataset\n",
    "    reference_key=\"reference\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755b38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-pertinent-payment-16' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/c04d309b-9751-4972-a901-b7864b48bdaa\n",
      "[--------->                                        ] 1/5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 3b626f03-d0db-4cbf-b085-2f0fb414d1fd with inputs {'question': \"What's my first meeting on Friday?\"}\n",
      "Error Type: ValidationError, Message: 1 validation error for check_calendarSchemaSchema\n",
      "date\n",
      "  invalid datetime format (type=value_error.datetime)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------------------->          ] 4/5"
     ]
    }
   ],
   "source": [
    "from langchain.smith import (\n",
    "    arun_on_dataset,\n",
    "    run_on_dataset, \n",
    ")\n",
    "\n",
    "chain_results = run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=agent_factory,\n",
    "    evaluation=evaluation_config,\n",
    "    verbose=True,\n",
    "    client=client,\n",
    "    # project_name=f\"runnable-agent-test-5d466cbc-0130\",\n",
    "    tags=[\"testing-notebook\", \"prompt:5d466cbc\"],  # Optional, adds a tag to the resulting chain runs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e70b867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9364934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c4359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
