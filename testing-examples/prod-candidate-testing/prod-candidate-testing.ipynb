{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776d4494-515a-4e5c-b146-515d4ecc8981",
   "metadata": {},
   "source": [
    "# Prod candidate testing\n",
    "\n",
    "Deploying your app into production is just one step on a longer journey of continuous improvement.\n",
    "\n",
    "You'll likely develop candidate improvements you want to test against your prod sytstem.\n",
    "\n",
    "This is somewhat analogous to \"back-testing\" (I guess), though you often won't have ground-truth labels in this case. (If your application DOES permit capturing ground-truth labels, then we obviously recommend you use those.\n",
    "\n",
    "This notebook shows how to do this in LangSmith.\n",
    "\n",
    "Basic steps are:\n",
    "\n",
    "1. Sample prod runs to test against.\n",
    "2. Convert runs to dataset + initial test.\n",
    "3. Run new system against the dataset to compare.\n",
    "\n",
    "In this way, each sample you may take would become a new dataset you can version and backtest against."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce385e04-67b8-4fa5-8024-265cf7303fd6",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Install + set environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac5a07f-97fe-48e3-9eb9-d5975364d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain langchain_anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce200938-9229-4b95-b711-e1bcb6a2d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the project name to whichever project you'd like to be testing against\n",
    "project_name = \"Tweet Critic\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR ANTHROPIC API KEY\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837af068-7ae6-497e-a29d-e677b637fe90",
   "metadata": {},
   "source": [
    "#### (Prelim) Production Deployment\n",
    "\n",
    "You likely have a project already and can skip this step. We'll simulate one here for the sake of the notebook. Our example app is a \"tweet critic\" that revises tweets we put out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "863adc9d-b4de-4e29-9e8f-50eaa4f52388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = hub.pull(\"wfh/tweet-critic:7e4f539e\")\n",
    "llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "system = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "inputs = [\n",
    "    \"\"\"RAG From Scratch: Our RAG From Scratch video series covers some important RAG concepts in short, focused videos with code. This is the 10th video and it discusses query routing. Problem: We sometimes have multiple datastores (e.g., different vector DBs, SQL DBs, etc) and prompts to choose from based on a user query. Idea: Logical routing can use an LLM to decide which datastore is more appropriate. Semantic routing embeds the query and prompts, then chooses the best prompt based on similarity. Video: https://youtu.be/pfpIndq7Fi8 Code: https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_10_and_11.ipynb\"\"\",\n",
    "    \"\"\"@Voyage_AI_ Embedding Integration Package Use the same custom embeddings that power Chat LangChain via the new langchain-voyageai package! Voyage AI builds custom embedding models that can improve retrieval quality. ChatLangChain: https://chat.langchain.com Python Docs: https://python.langchain.com/docs/integrations/providers/voyageai\"\"\",\n",
    "    \"\"\"Implementing RAG: How to Write a Graph Retrieval Query in LangChain Our friends at @neo4j have a nice guide on combining LLMs and graph databases. Blog:\"\"\",\n",
    "    \"\"\"Text-to-PowerPoint with LangGraph.js You can now generate PowerPoint presentations from text! @TheGreatBonnie wrote a guide showing how to use LangGraph.js, @tavilyai, and @CopilotKit to build a Next.js app for this. Tutorial: https://dev.to/copilotkit/how-to-build-an-ai-powered-powerpoint-generator-langchain-copilotkit-openai-nextjs-4c76 Repo: https://github.com/TheGreatBonnie/aipoweredpowerpointapp\"\"\",\n",
    "    \"\"\"Build an Answer Engine Using Groq, Mixtral, Langchain, Brave & OpenAI in 10 Min Our friends at @Dev__Digest have a tutorial on building an answer engine over the internet. Code: https://github.com/developersdigest/llm-answer-engine YouTube: https://youtube.com/watch?v=43ZCeBTcsS8&t=96s\"\"\",\n",
    "    \"\"\"Building a RAG Pipeline with LangChain and Amazon Bedrock Amazon Bedrock has great models for building LLM apps. This guide covers how to get started with them to build a RAG pipeline. https://gettingstarted.ai/langchain-bedrock/\"\"\",\n",
    "    \"\"\"SF Meetup on March 27! Join our meetup to hear from LangChain and Pulumi experts and learn about building AI-enabled capabilities. Sign up: https://meetup.com/san-francisco-pulumi-user-group/events/299491923/?utm_campaign=FY2024Q3_Meetup_PUG%20SF&utm_content=286236214&utm_medium=social&utm_source=twitter&hss_channel=tw-837770064870817792\"\"\",\n",
    "    \"\"\"Chat model response metadata @LangChainAI chat model invocations now include metadata like logprobs directly in the output. Upgrade your version of `langchain-core` to try it. PY: https://python.langchain.com/docs/modules/model_io/chat/logprobs JS: https://js.langchain.com/docs/integrations/chat/openai#generation-metadata\"\"\",\n",
    "    \"\"\"Benchmarking Query Analysis in High Cardinality Situations Handling high-cardinality categorical values can be challenging. This blog explores 6 different approaches you can take in these situations. Blog: https://blog.langchain.dev/high-cardinality\"\"\",\n",
    "    \"\"\"Building Google's Dramatron with LangGraph.js & Claude 3 We just released a long YouTube video (1.5 hours!) on building Dramatron using LangGraphJS and @AnthropicAI's Claude 3 \"Haiku\" model. It's a perfect fit for LangGraph.js and Haiku's speed. Check out the tutorial: https://youtube.com/watch?v=alHnQjyn7hg\"\"\",\n",
    "    \"\"\"Document Loading Webinar with @AirbyteHQ Join a webinar on document loading with PyAirbyte and LangChain on 3/14 at 10am PDT. Features our founding engineer @eyfriis and the @aaronsteers and Bindi Pankhudi team. Register: https://airbyte.com/session/airbyte-monthly-ai-demo\"\"\",\n",
    "]\n",
    "\n",
    "_ = system.batch(\n",
    "    [{\"messages\": [HumanMessage(content=content)]} for content in inputs],\n",
    "    {\"max_concurrency\": 3},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5899ef2f-8846-4449-9eb1-a4e5e58db76a",
   "metadata": {},
   "source": [
    "## Convert Prod Runs to Test\n",
    "\n",
    "The first step is to generate a dataset based on the production _inputs_.\n",
    "Then copy over all the traces to serve as a baseline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d8f494b-e1a3-4a1d-81d2-805f3307ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "SAMPLE_SIZE = 10\n",
    "end_time = datetime.now(tz=timezone.utc)\n",
    "start_time = end_time - timedelta(days=1)\n",
    "filter = f'and(gt(start_time, \"{start_time.isoformat()}\"), lt(end_time, \"{end_time.isoformat()}\"))'\n",
    "dataset_name = f'{project_name}-candidate-testing {start_time.strftime(\"%Y-%m-%d\")}-{end_time.strftime(\"%Y-%m-%d\")}'\n",
    "\n",
    "# This will make it a bit slower, but will let you view the full trace\n",
    "include_children = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb6bc59-3456-44d1-a5a8-08b80ad69a88",
   "metadata": {},
   "source": [
    "#### Run conversion script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18de1343-d9b4-4ffc-b583-e963c497f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_dataset(dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9856567-02dd-42b8-896d-c92f23a822b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import uuid\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "prod_runs = list(\n",
    "    client.list_runs(\n",
    "        project_name=project_name,\n",
    "        execution_order=1,\n",
    "        filter=filter,\n",
    "    )\n",
    ")\n",
    "\n",
    "# We will downsample to only N runs\n",
    "sampled_runs = random.sample(prod_runs, min(SAMPLE_SIZE, len(prod_runs)))\n",
    "\n",
    "# Then convert each run's inputs to an example input\n",
    "ds = client.create_dataset(dataset_name=dataset_name)\n",
    "client.create_examples(\n",
    "    inputs=[r.inputs for r in sampled_runs],\n",
    "    source_run_ids=[r.id for r in sampled_runs],\n",
    "    dataset_id=ds.id,\n",
    ")\n",
    "\n",
    "# Now we'll copy over the prod runs as a \"test run\"\n",
    "\n",
    "if not include_children:\n",
    "    runs_to_copy = sampled_runs\n",
    "else:\n",
    "    # TODO: re-fetch with trace_id\n",
    "    runs_to_copy = [client.read_run(r.id, load_child_runs=True) for r in sampled_runs]\n",
    "\n",
    "\n",
    "# Copy over and update IDs\n",
    "\n",
    "\n",
    "test_project_name = f\"prod-baseline-{uuid.uuid4().hex[:6]}\"\n",
    "\n",
    "run_to_example_map = {\n",
    "    e.source_run_id: e.id for e in client.list_examples(dataset_name=dataset_name)\n",
    "}\n",
    "\n",
    "\n",
    "def convert_ids(run_dict: dict, id_map: dict):\n",
    "    # Convert dotted order and parent_run_id\n",
    "    do = run_dict[\"dotted_order\"]\n",
    "    # TODO: speed up / compile regex\n",
    "    for k, v in id_map.items():\n",
    "        do = do.replace(str(k), str(v))\n",
    "    run_dict[\"dotted_order\"] = do\n",
    "\n",
    "    # parent_run_id\n",
    "    if run_dict.get(\"parent_run_id\"):\n",
    "        run_dict[\"parent_run_id\"] = id_map[run_dict[\"parent_run_id\"]]\n",
    "    if not run_dict.get(\"extra\"):\n",
    "        run_dict[\"extra\"] = {}\n",
    "    return run_dict\n",
    "\n",
    "\n",
    "def convert_root_run(root):\n",
    "    # Mutate the trace_id and run_id, the dotted order, and the parent_run_ids.\n",
    "    runs_ = [root]\n",
    "    trace_id = uuid.uuid4()\n",
    "    id_map = {root.trace_id: trace_id}\n",
    "    results = []\n",
    "    while runs_:\n",
    "        src = runs_.pop()\n",
    "        src_dict = src.dict(exclude={\"parent_run_ids\", \"child_run_ids\", \"session_id\"})\n",
    "        id_map[src_dict[\"id\"]] = id_map.get(src_dict[\"id\"], uuid.uuid4())\n",
    "        src_dict[\"id\"] = id_map[src_dict[\"id\"]]\n",
    "        src_dict[\"trace_id\"] = id_map[src_dict[\"trace_id\"]]\n",
    "        if src.child_runs:\n",
    "            runs_.extend(src.child_runs)\n",
    "        results.append(src_dict)\n",
    "    result = [convert_ids(r, id_map) for r in results]\n",
    "    result[0][\"reference_example_id\"] = run_to_example_map[root.id]\n",
    "    return result\n",
    "\n",
    "\n",
    "to_create = [\n",
    "    run_dict for root_run in runs_to_copy for run_dict in convert_root_run(root_run)\n",
    "]\n",
    "\n",
    "project = client.create_project(\n",
    "    project_name=test_project_name,\n",
    "    reference_dataset_id=ds.id,\n",
    "    metadata={\n",
    "        \"system_version\": \"prod\",\n",
    "    },\n",
    ")\n",
    "# Copy modified runs over.\n",
    "for new_run in to_create:\n",
    "    client.create_run(**new_run, project_name=test_project_name)\n",
    "\n",
    "# Close out the project so you can manually modify the metadata attributes if desired\n",
    "_ = client.update_project(project.id, end_time=datetime.now(tz=timezone.utc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ac5e60-1210-4f9e-a8b7-c14d9c3d40ee",
   "metadata": {},
   "source": [
    "## Run new system\n",
    "\n",
    "Now we have the dataset and prod runs saved as a \"test\".\n",
    "\n",
    "Let's run inference on our new system to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd2ab544-2791-495b-ada9-33f67b05c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an updated version of the prompt\n",
    "prompt = hub.pull(\"wfh/tweet-critic:34c57e4f\")\n",
    "llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "system = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c73d53ac-ac50-4cdf-b9cb-b2de7320d6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'memorable-disease-42' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/a41492bc-da4d-4379-8199-fd4e8877e01a/compare?selectedSessions=93951de1-ea67-4e65-af98-daec016d711c\n",
      "\n",
      "View all tests for Dataset Tweet Critic-candidate-testing 2024-03-17-2024-03-18 at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/a41492bc-da4d-4379-8199-fd4e8877e01a\n",
      "[------------------------------------------------->] 10/10"
     ]
    }
   ],
   "source": [
    "from langchain.load import load\n",
    "\n",
    "\n",
    "def deserialize_messages(example_input: dict):\n",
    "    # The dataset includes serialized messages that we\n",
    "    # must convert to a format accepted by our system.\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            (message[\"type\"], message[\"content\"])\n",
    "            for message in example_input[\"messages\"]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "test_results = client.run_on_dataset(\n",
    "    llm_or_chain_factory=deserialize_messages | system,\n",
    "    dataset_name=dataset_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f441f7-ba76-4278-b60e-4ddc7a082461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
