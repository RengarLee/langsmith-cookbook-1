{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a33c67",
   "metadata": {},
   "source": [
    "# Comparing Q&A System Outputs\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langsmith-cookbook/blob/master/./testing-examples/comparing-runs/comparing-qa.ipynb)\n",
    "\n",
    "The most common way to compare two models is to benchmark them both on a dataset and compare the aggregate metrics.\n",
    "\n",
    "This approach is useful but it may filter out helpful information about the quality of the two system variants. In this case, it can be helpful to directly perform pairwise comparisons on the responses and take the resulting preference scores into consideration.\n",
    "\n",
    "In this tutorial, we will share one way to do this in code. We will use a retrieval Q&A system over LangSmith's docs as a motivating example.\n",
    "\n",
    "The main steps are:\n",
    "\n",
    "1. Setup\n",
    "   - Create a dataset of questions and answers.\n",
    "   - Define different versions of your chains to evaluate.\n",
    "   - Evaluate chains directly on a dataset using regular metrics (e.g. correctness).\n",
    "4. Evaluate the pairwise preferences over that dataset\n",
    "\n",
    "In this case, we will test the impact of chunk sizes on our result quality. Let's begin!\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This tutorial uses OpenAI for the model, ChromaDB to store documents, and LangChain to compose the chain. To make sure the tracing and evals are set up for [LangSmith](https://smith.langchain.com), please configure your API Key appropriately.\n",
    "\n",
    "We will also use pandas to render the results in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c788783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:21.905597Z",
     "start_time": "2023-09-20T04:50:21.886104Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\" # Update with your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe780fbd",
   "metadata": {},
   "source": [
    "Install the required packages. `lxml` and `html2text` are used by the document loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9e7425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:23.269252Z",
     "start_time": "2023-09-20T04:50:23.265495Z"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install -U \"langchain[openai]\" --quiet\n",
    "# %pip install chromadb --quiet\n",
    "# %pip install lxml --quiet\n",
    "# %pip install html2text --quiet\n",
    "# %pip install pandas --quiet\n",
    "# %pip install nest_asyncio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afac8079",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:25.057984Z",
     "start_time": "2023-09-20T04:50:25.050240Z"
    }
   },
   "outputs": [],
   "source": [
    "# %env OPENAI_API_KEY=<YOUR-API-KEY>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bae821d-1861-4b43-9495-7285326c6401",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:25.617651Z",
     "start_time": "2023-09-20T04:50:25.606240Z"
    }
   },
   "outputs": [],
   "source": [
    "# Used for running in jupyter\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80ab6e",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "#### a. Create a dataset\n",
    "\n",
    "No evaluation process is complete without a development dataset. We've hard-coded a few examples below to demonstrate the process. In general, you'll want a lot more (>100) pairs for statistically significant results. Drawing from actual user queries can be helpful to ensure better representation of the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e98a80-bf37-457e-b31d-952292e76c51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:27.908227Z",
     "start_time": "2023-09-20T04:50:27.903470Z"
    }
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    (\"What is LangChain?\", \"LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\"),\n",
    "    (\"How might I query for all runs in a project?\", \"client.list_runs(project_name='my-project-name'), or in TypeScript, client.ListRuns({projectName: 'my-project-anme'})\"),\n",
    "    (\"What's a langsmith dataset?\", \"A LangSmith dataset is a collection of examples. Each example contains inputs and optional expected outputs or references for that data point.\"),\n",
    "    (\"How do I use a traceable decorator?\", \"\"\"The traceable decorator is available in the langsmith python SDK. To use, configure your environment with your API key,\\\n",
    "import the required function, decorate your function, and then call the function. Below is an example:\n",
    "```python\n",
    "from langsmith.run_helpers import traceable\n",
    "@traceable(run_type=\"chain\") # or \"llm\", etc.\n",
    "def my_function(input_param):\n",
    "    # Function logic goes here\n",
    "    return output\n",
    "result = my_function(input_param)\n",
    "```\"\"\"),\n",
    "    (\"Can I trace my Llama V2 llm?\", \"So long as you are using one of LangChain's LLM implementations, all your calls can be traced\"),\n",
    "    (\"Why do I have to set environment variables?\", \"Environment variables can tell your LangChain application to perform tracing and contain the information necessary to authenticate to LangSmith.\"\n",
    "     \" While there are other ways to connect, environment variables tend to be the simplest way to configure your application.\"),\n",
    "    (\"How do I move my project between organizations?\", \"LangSmith doesn't directly support moving projects between organizations.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5edb7824",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:28.911950Z",
     "start_time": "2023-09-20T04:50:28.903935Z"
    }
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbcd3690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:31.532293Z",
     "start_time": "2023-09-20T04:50:29.899253Z"
    }
   },
   "outputs": [],
   "source": [
    "import uuid \n",
    "\n",
    "dataset_name = f\"Retrieval QA Questions {str(uuid.uuid4())}\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "for q, a in examples:\n",
    "    client.create_example(inputs={\"question\": q}, outputs={\"answer\": a}, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976437f",
   "metadata": {},
   "source": [
    "#### b. Define RAG Q&A system\n",
    "\n",
    "Our Q&A system uses a simple retriever and LLM response generator. To break that down further, the chain will be composed of:\n",
    "\n",
    "1. A [VectorStoreRetriever](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.base.VectorStoreRetriever.html#langchain.vectorstores.base.VectorStoreRetriever) to retrieve documents. This uses:\n",
    "   - An embedding model to vectorize documents and user queries for retrieval. In this case, the [OpenAIEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html) model.\n",
    "   - A vectorstore, in this case we will use [Chroma](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.chroma.Chroma.html#langchain.vectorstores.chroma.Chroma).\n",
    "2. A response generator. This uses:\n",
    "   - A [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.chat.ChatPromptTemplate.html#langchain.prompts.chat.ChatPromptTemplate) to combine the query and documents. \n",
    "   - An LLM, in this case, the 16k token context window version of `gpt-3.5-turbo` via [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html#langchain.chat_models.openai.ChatOpenAI).\n",
    "\n",
    "We will combine them using LangChain's [expression syntax](https://python.langchain.com/docs/guides/expression_language/cookbook).\n",
    "\n",
    "First, load the documents to populate the vectorstore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95fab721",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:47.262606Z",
     "start_time": "2023-09-20T04:50:36.292760Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mukilloganathan/langchain/venv/lib/python3.11/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import RecursiveUrlLoader\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "api_loader = RecursiveUrlLoader(\"https://docs.smith.langchain.com\")\n",
    "doc_transformer = Html2TextTransformer()\n",
    "raw_documents = api_loader.load()\n",
    "transformed = doc_transformer.transform_documents(raw_documents)\n",
    "\n",
    "def create_retriever(transformed_documents, text_splitter):\n",
    "    documents = text_splitter.split_documents(transformed_documents)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e35f884-bb93-427d-a0ad-3858c449a1ea",
   "metadata": {},
   "source": [
    "Next up, we'll define the chain. Since we are going to vary the retriever parameters, our constructor will\n",
    "take the retriever as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2f266e9-e4de-42ad-b41e-99ace4dc5131",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:47.264432Z",
     "start_time": "2023-09-20T04:50:47.263828Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "def create_chain(retriever):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", \"You are a helpful documentation Q&A assistant, trained to answer\"\n",
    "                \" questions from LangSmith's documentation.\"\n",
    "                \" LangChain is a framework for building applications using large language models.\"\n",
    "                \"\\nThe current time is {time}.\\n\\nRelevant documents will be retrieved in the following messages.\"),\n",
    "                (\"system\", \"{context}\"),\n",
    "                (\"human\",\"{question}\")\n",
    "            ]\n",
    "        ).partial(time=str(datetime.now()))\n",
    "\n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "    response_generator = (\n",
    "        prompt \n",
    "        | model \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    chain = (\n",
    "        # The runnable map here routes the original inputs to a context and a question dictionary to pass to the response generator\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | retriever | (lambda docs: \"\\n\".join([doc.page_content for doc in docs])),\n",
    "            \"question\": itemgetter(\"question\")\n",
    "        }\n",
    "        | response_generator\n",
    "    )\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d02a25-2863-4c5c-81e8-00887247516d",
   "metadata": {},
   "source": [
    "With the documents prepared, and the chain constructor ready, it's time to create and evaluate our chains.\n",
    "We will vary the split size and overlap to evaluate its impact on the response quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1028d963-d84b-4759-a5ca-087b27065485",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T04:50:47.265315Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "retriever = create_retriever(transformed, text_splitter)\n",
    "\n",
    "chain_1 = create_chain(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5765c938-c5f6-4ee4-be6b-90b7b341b683",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T04:50:47.266475Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will shrink both the chunk size and overlap\n",
    "text_splitter_2 = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "retriever_2 = create_retriever(transformed, text_splitter_2)\n",
    "\n",
    "chain_2 = create_chain(retriever_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dddd61",
   "metadata": {},
   "source": [
    "#### c. Evaluate the chains\n",
    "\n",
    "At this point, we are still going through the regular development -> evaluation process. We have two candidates and will evaluate them with a correctness evaluator from LangChain. By running `run_on_dataset`, we will generate predicted answers to each question in the dataset and log feedback from the evaluator for that data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62de3aea-638e-4add-998a-22eb21f6feaf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T04:50:47.267369Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    # We will use the chain-of-thought Q&A correctness evaluator\n",
    "    evaluators=[\"cot_qa\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ae51a83-6ae1-4fa6-a9d8-4e27bf03047f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T04:50:47.268056Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-terrific-sneeze-7' at:\n",
      "https://smith.langchain.com/o/9a6371ef-ea6a-4860-b3bd-9614084873e7/projects/p/86b4ea18-3749-4c56-99bf-e1399541a0d7\n",
      "[------------------------------------------------->] 7/7"
     ]
    }
   ],
   "source": [
    "results = client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=chain_1,\n",
    "    evaluation=eval_config\n",
    ")\n",
    "project_name = results[\"project_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec375e76-7261-433b-9f02-f4750acf6f93",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-20T04:50:47.268715Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-warm-lettuce-8' at:\n",
      "https://smith.langchain.com/o/9a6371ef-ea6a-4860-b3bd-9614084873e7/projects/p/4980a6ab-15df-476e-ac8a-fcf35adc8ba1\n",
      "[------------------------------------------------->] 7/7"
     ]
    }
   ],
   "source": [
    "results_2 = client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=chain_2,\n",
    "    evaluation=eval_config\n",
    ")\n",
    "project_name_2 = results_2[\"project_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d2019-d076-4f18-832e-634dbd31091c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now you should have two test run projects over the same dataset. If you click on one, it should look something like the following:\n",
    "    \n",
    "![Original Feedback](img/original_eval.png)\n",
    "\n",
    "You can look at the aggregate results here and for the other project to compare them. You could also view them in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48246571-35e5-4e16-9ea0-cdeb499d5bd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:51.452128Z",
     "start_time": "2023-09-20T04:50:51.442453Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to wrap the results in the dataframe table\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "def word_wrap_on_hover(df):\n",
    "    styles = \"\"\"\n",
    "    <style>\n",
    "        .hover_table td {\n",
    "            max-width: 200px; /* You can adjust this value */\n",
    "            overflow: hidden;\n",
    "            text-overflow: ellipsis;\n",
    "            white-space: nowrap;\n",
    "        }\n",
    "        .hover_table td:hover {\n",
    "            white-space: normal;\n",
    "            word-wrap: break-word;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    html_table = df.to_html(index=False, classes='hover_table')\n",
    "    return HTML(styles + html_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f50f74cc-fc86-4c60-8431-70695eba7aac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:52.710833Z",
     "start_time": "2023-09-20T04:50:51.723217Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "runs_1 = list(client.list_runs(project_name=project_name, execution_order=1))\n",
    "runs_2 = list(client.list_runs(project_name=project_name_2, execution_order=1))\n",
    "\n",
    "def get_project_df(runs):\n",
    "    return pd.DataFrame([{**run.outputs, **{k: v.get('avg') for k, v in run.feedback_stats.items()}} for run in runs], index = [run.reference_example_id for run in runs])\n",
    "\n",
    "runs_1_df = get_project_df(runs_1)\n",
    "runs_2_df = get_project_df(runs_2)\n",
    "joined_df = runs_1_df.join(runs_2_df, lsuffix='_1', rsuffix='_2')\n",
    "columns_1 = [col for col in joined_df.columns if col.endswith('_1')]\n",
    "columns_2 = [col for col in joined_df.columns if col.endswith('_2')]\n",
    "new_columns_order = [col for pair in zip(columns_1, columns_2) for col in pair]\n",
    "joined_df = joined_df[new_columns_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45e3d81d-b16b-4ba2-bf4b-1118973e30d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:50:53.133194Z",
     "start_time": "2023-09-20T04:50:53.122750Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .hover_table td {\n",
       "            max-width: 200px; /* You can adjust this value */\n",
       "            overflow: hidden;\n",
       "            text-overflow: ellipsis;\n",
       "            white-space: nowrap;\n",
       "        }\n",
       "        .hover_table td:hover {\n",
       "            white-space: normal;\n",
       "            word-wrap: break-word;\n",
       "        }\n",
       "    </style>\n",
       "    <table border=\"1\" class=\"dataframe hover_table\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>output_1</th>\n",
       "      <th>output_2</th>\n",
       "      <th>COT Contextual Accuracy_1</th>\n",
       "      <th>COT Contextual Accuracy_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LangChain is a framework for building applications using large language models (LLMs). It simplifies the process of setting up and using LLMs, such as OpenAI's GPT models, by providing a set of tools and utilities. LangChain aims to bridge the gap between the initial setup of LLMs and their reliable use in production applications. It includes features like tracing, debugging, testing, evaluation, and monitoring to help developers build and improve LLM applications.</td>\n",
       "      <td>LangChain is a framework developed by LangChain, Inc. for building applications using large language models. It provides a set of tools and libraries that enable developers to leverage the power of language models in their applications. With LangChain, developers can easily integrate language models into their projects, allowing them to perform tasks such as natural language understanding, text generation, sentiment analysis, and more. The framework supports various programming languages and provides a range of features to simplify the development process.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>To query for all runs in a project, you can use the LangSmith client's `list_runs` method. Here's an example in Python:\\n\\n```python\\nfrom langsmith import Client\\n\\nclient = Client()\\nruns = list(client.list_runs(project_name=\"&lt;your_project&gt;\"))\\n```\\n\\nIn this example, replace `&lt;your_project&gt;` with the name of your project. The `list_runs` method returns a generator that you can iterate over to access each run in the project.\\n\\nIf you're using TypeScript, you can use a similar approach:\\n\\n```typescript\\nimport { Client, Run } from \"langsmith\";\\n\\nconst client = new Client();\\nconst runs: Run[] = [];\\nfor await (const run of client.listRuns({ projectName: \"&lt;your_project&gt;\" })) {\\n  runs.push(run);\\n}\\n```\\n\\nAgain, replace `&lt;your_project&gt;` with the name of your project. The `listRuns` method returns an asynchronous iterable that you can loop over to access each run in the project.</td>\n",
       "      <td>To query for all runs in a project using the LangSmith API, you can use the `list_runs` method of the `Client` object. Here's an example in Python:\\n\\n```python\\nfrom langsmith import Client\\n\\nclient = Client()\\nproject_runs = client.list_runs(project_name=\"&lt;your_project&gt;\")\\n```\\n\\nThis will retrieve all runs in the specified project. You can replace `&lt;your_project&gt;` with the name of your project.\\n\\nIn TypeScript, you can use the `listRuns` method of the `Client` class. Here's an example:\\n\\n```typescript\\nimport { Client } from \"langsmith\";\\n\\nconst client = new Client();\\nconst projectRuns = await client.listRuns({ projectName: \"&lt;your_project&gt;\" });\\n```\\n\\nAgain, replace `&lt;your_project&gt;` with the name of your project.\\n\\nThe `list_runs` or `listRuns` method returns a list of `Run` objects that represent the runs in the project. You can iterate over this list to access the individual runs and their properties.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Yes, you can trace your Llama V2 LLM by enabling tracing in your LangChain application. Tracing allows you to log the inputs and outputs of each component in your LLM application, including the Llama V2 LLM. This can be helpful for debugging and understanding the behavior of your LLM.\\n\\nTo enable tracing, you need to set the `LANGCHAIN_TRACING_V2` environment variable to `true` before running your LangChain application. This will ensure that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith.\\n\\nHere's an example of how to enable tracing in Python:\\n\\n```python\\nimport os\\nfrom langchain.chat_models import LlamaV2\\n\\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\\n\\nllm = LlamaV2()\\nllm.invoke(\"Hello, world!\")\\n```\\n\\nAnd here's an example in TypeScript:\\n\\n```typescript\\nimport { LlamaV2 } from \"langchain/chat_models/llama_v2\";\\n\\nprocess.env.LANGCHAIN_TRACING_V2 = \"true\";\\n\\nconst llm = new LlamaV2();\\nawait llm.invoke(\"Hello, world!\");\\n```\\n\\nRemember to replace `\"Hello, world!\"` with your desired input to the LLM.</td>\n",
       "      <td>Yes, you can trace your Llama V2 LLM using LangSmith. LangSmith's tracing feature allows you to log and visualize the inputs, outputs, and sequence of events for LLM calls, including Llama V2. This can be helpful for debugging, understanding the exact input to the LLM, analyzing the sequence of events, identifying slow components, and tracking token usage. You can enable tracing for Llama V2 LLM calls by setting the appropriate environment variables or using the LangChainTracer callback when initializing the LLM.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>The `traceable` decorator is a convenient way to log your function calls as runs in LangSmith. It automatically captures the inputs, outputs, and metadata of the function and sends them to LangSmith for logging.\\n\\nTo use the `traceable` decorator, follow these steps:\\n\\n1. Import the `traceable` decorator from the `langsmith.run_helpers` module.\\n2. Decorate your function with `@traceable(run_type=\"chain\")`, specifying the `run_type` as \"chain\" (or any other appropriate value).\\n3. Call your decorated function as usual.\\n\\nHere's an example in Python:\\n\\n```python\\nfrom langsmith.run_helpers import traceable\\n\\n@traceable(run_type=\"chain\")\\ndef my_function(input1, input2):\\n    # Your function logic here\\n    output = input1 + input2\\n    return output\\n\\nresult = my_function(3, 4)\\n```\\n\\nIn this example, the `my_function` is decorated with `@traceable(run_type=\"chain\")`. When the function is called, the inputs, outputs, and metadata will be logged as a run in LangSmith.\\n\\nNote that you can customize the `run_type` to match the type of operation your function represents (e.g., \"llm\", \"tool\", etc.). You can also pass additional metadata to the decorator using the `metadata` parameter.\\n\\nMake sure you have set up the LangSmith SDK and configured the necessary environment variables before using the `traceable` decorator.</td>\n",
       "      <td>To use the `traceable` decorator in LangSmith, you can follow these steps:\\n\\n1. Import the necessary modules:\\n   - For Python:\\n     ```python\\n     from langsmith.run_helpers import traceable\\n     ```\\n   - For TypeScript:\\n     ```typescript\\n     import { traceable } from \"langsmith/run_helpers\";\\n     ```\\n\\n2. Decorate the function you want to trace with the `traceable` decorator. This decorator will automatically create a run tree for the function and log it to LangSmith. You can specify the `run_type` and `name` parameters to customize the type and name of the run. For example:\\n   - For Python:\\n     ```python\\n     @traceable(run_type=\"chain\", name=\"my_chain_function\")\\n     def my_chain_function(input_data):\\n         # Function logic here\\n         return output_data\\n     ```\\n   - For TypeScript:\\n     ```typescript\\n     @traceable({ run_type: \"chain\", name: \"my_chain_function\" })\\n     function myChainFunction(inputData: any): any {\\n         // Function logic here\\n         return outputData;\\n     }\\n     ```\\n\\n3. Call the traced function as usual. The function execution will be logged to LangSmith.\\n\\nBy using the `traceable` decorator, you can easily log the execution of your functions and visualize the call hierarchy in the LangSmith app.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>A LangSmith dataset is a collection of examples that can be used to evaluate or improve a chain, agent, or model built using the LangSmith framework. Each example in a dataset consists of inputs and (optionally) expected outputs for a given interaction. \\n\\nThere are three types of datasets in LangSmith:\\n\\n1. Key-value datasets: These datasets allow for arbitrary key-value pairs as inputs and outputs. They are useful when evaluating chains and agents that require multiple inputs or return multiple outputs.\\n\\n2. LLM datasets: These datasets correspond to string inputs and outputs from \"completion\" style language models (LLMs). The inputs and outputs are simple strings.\\n\\n3. Chat datasets: These datasets correspond to structured \"chat\" messages as inputs and outputs. Each example row expects a list of serialized chat messages as inputs and outputs.\\n\\nDatasets can be managed and created using the LangSmith web app or through the LangSmith client library. Examples can be added to datasets from existing runs, uploaded from CSV files, or created programmatically using the client library. Datasets can also be exported to CSV or OpenAI evals format for further analysis or sharing.</td>\n",
       "      <td>A LangSmith dataset is a collection of input-output examples that are used for testing and evaluation purposes. It is a way to organize and store representative runs from your debug projects. These examples can be added to a dataset using the \"Add to Dataset\" button in LangSmith.\\n\\nDatasets are valuable for benchmarking and comparing different versions of your chains or prompts. They provide a set of inputs and expected outputs that can be used to test the performance and accuracy of your models. Datasets can be used to evaluate changes to prompts or chains, and they can also be shared with others for collaborative debugging or analysis.\\n\\nIn LangSmith, datasets can be easily uploaded and managed. Once a dataset is created, you can run your chains over the data points and log the results to a new project associated with the dataset. You can review the results, assign feedback to runs, and mark them as correct or incorrect. Datasets can be a powerful tool for testing, evaluating, and improving your language model applications.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Currently, LangSmith does not support project migration between organizations. If you want to move your project to a different organization, the recommended approach is to create a new project within the desired organization and manually transfer the necessary data, such as runs and datasets, to the new project.\\n\\nYou can achieve this by using the LangSmith SDK. Here's an example of how you can export runs from one project and import them into another project:\\n\\n1. Export runs from the source project:\\n   ```python\\n   from langsmith import Client\\n\\n   client = Client()\\n   runs = list(client.list_runs(project_name=\"&lt;source_project&gt;\"))\\n\\n   # Save the runs to a file or perform any necessary processing\\n   # ...\\n\\n   ```\\n\\n2. Import runs into the destination project:\\n   ```python\\n   from langsmith import Client\\n\\n   client = Client()\\n   with open(\"&lt;runs_file&gt;\", \"r\") as file:\\n       runs_data = file.read()\\n\\n   # Create the runs in the destination project\\n   client.create_runs(project_name=\"&lt;destination_project&gt;\", runs_data=runs_data)\\n   ```\\n\\nYou can follow a similar approach for transferring datasets or any other data associated with your project.\\n\\nPlease note that this process requires manual intervention and may take some time depending on the size and complexity of your project.</td>\n",
       "      <td>Currently, LangSmith does not support project migration between organizations. However, you can manually export and import runs and datasets using the SDK to imitate project migration. Here's how you can do it:\\n\\n1. Export Runs: Use the LangSmith SDK to export the runs from your current organization. You can use the `export_run` function to export individual runs or the `export_runs` function to export multiple runs at once. Save the exported runs to a file.\\n\\n2. Export Datasets: Similarly, use the LangSmith SDK to export the datasets from your current organization. You can use the `export_dataset` function to export individual datasets or the `export_datasets` function to export multiple datasets at once. Save the exported datasets to a file.\\n\\n3. Create a New Project: In your new organization, create a new project where you want to import the runs and datasets.\\n\\n4. Import Runs: Use the LangSmith SDK to import the exported runs into the new project. You can use the `import_run` function to import individual runs or the `import_runs` function to import multiple runs at once. Provide the path to the exported runs file.\\n\\n5. Import Datasets: Similarly, use the LangSmith SDK to import the exported datasets into the new project. You can use the `import_dataset` function to import individual datasets or the `import_datasets` function to import multiple datasets at once. Provide the path to the exported datasets file.\\n\\nBy following these steps, you can effectively move your project from one organization to another.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Setting environment variables is necessary in order to configure and customize the behavior of LangSmith. Environment variables provide a way to pass configuration information to the LangSmith framework without hardcoding it in your code. This allows for flexibility and easy configuration changes without modifying the code itself.\\n\\nIn the context of LangSmith, setting environment variables is particularly important for enabling tracing and logging functionality. By setting the appropriate environment variables, you can enable LangSmith to log all calls to LLMs, chains, agents, tools, and retrievers. This logging is essential for debugging and understanding the behavior of your application.\\n\\nAdditionally, environment variables are used to configure other aspects of LangSmith, such as API keys for accessing the LangChain Hub and other services. By setting these variables, you can securely authenticate and access the necessary resources.\\n\\nOverall, setting environment variables is a best practice for configuring and customizing the behavior of LangSmith and ensuring smooth operation of your application.</td>\n",
       "      <td>Setting environment variables is necessary in order to provide configuration information to your application. Environment variables are used to store sensitive information, such as API keys or project names, that should not be hard-coded in your codebase. By using environment variables, you can easily manage and update these configurations without modifying your code.\\n\\nIn the context of the LangSmith framework, setting environment variables allows you to configure important information such as your API key, project name, and endpoint. These variables are used by the LangSmith SDK to authenticate and interact with the LangSmith API.\\n\\nOverall, using environment variables for configuration provides a more secure and flexible approach to managing sensitive information in your applications.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_wrap_on_hover(joined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b16bf4d-383b-4f44-b5bd-b56c42b91919",
   "metadata": {
    "tags": []
   },
   "source": [
    "It looks like the benchmark performance is similar, so let's move on to the pairwise comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb19f4-1855-4e9c-a447-491625c66646",
   "metadata": {},
   "source": [
    "## 2. Pairwise Evaluation\n",
    "\n",
    "Suppose both approaches return similar scores when evaluated in isolation.\n",
    "\n",
    "We can run a pairwise evaluator to see how try to predict preferred outputs. We will first define a couple helper functions to run the evaluator\n",
    "on each prediction pair. Let's break this function down:\n",
    "\n",
    "- The function accepts a dataset example and loads each model's predictions on that data point.\n",
    "- It then randomizes the order of the predictions and calls the evaluator. This is done to aveage out the impact of any ordering bias in the evaluator LLM.\n",
    "- Once the evaluation result is returned, we check it to make sure it is valid and then log feedback for both models.\n",
    "\n",
    "Once this is complete, the values are all returned so we can display them in a table in the notebook below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf7c70f3-9c8f-436d-85ca-5a398c3b1369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import logging\n",
    "\n",
    "def _get_run_and_prediction(example_id, project_name):\n",
    "    run = next(client.list_runs(reference_example_id=example_id, project_name=project_name, execution_order=1))\n",
    "    prediction = next(iter(run.outputs.values()))\n",
    "    return run, prediction\n",
    "\n",
    "def _log_feedback(run_ids):\n",
    "    for score, run_id in enumerate(run_ids):\n",
    "        client.create_feedback(run_id, key=\"preference\", score=score)\n",
    "\n",
    "def predict_preference(example, project_a, project_b, eval_chain):\n",
    "    example_id = example.id\n",
    "    print(example)\n",
    "    run_a, pred_a = _get_run_and_prediction(example_id, project_a)\n",
    "    run_b, pred_b = _get_run_and_prediction(example_id, project_b)\n",
    "    input_, answer = example.inputs['question'], example.outputs['answer']\n",
    "    result = {\"input\": input_, \"answer\": answer, \"A\": pred_a, \"B\": pred_b}\n",
    "\n",
    "    # Flip a coin to average out persistent positional bias\n",
    "    if random.random() < 0.5:\n",
    "        result['A'], result['B'] = result['B'], result['A']\n",
    "        run_a, run_b = run_b, run_a\n",
    "    try:\n",
    "        eval_res = eval_chain.evaluate_string_pairs(\n",
    "            prediction=result['A'],\n",
    "            prediction_b=result['B'],\n",
    "            input=input_, \n",
    "            reference=answer\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.warning(e)\n",
    "        return result\n",
    "\n",
    "    if eval_res[\"value\"] is None:\n",
    "        return result\n",
    "\n",
    "    preferred_run = (run_a.id, \"A\") if eval_res[\"value\"] == \"A\" else (run_b.id, \"B\")\n",
    "    runner_up_run = (run_b.id, \"B\") if eval_res[\"value\"] == \"A\" else (run_a.id, \"A\")\n",
    "    _log_feedback((runner_up_run[0], preferred_run[0]))\n",
    "    result[\"Preferred\"] = preferred_run[1]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6a51df-616f-4f3e-9106-a5f7669b9090",
   "metadata": {},
   "source": [
    "For this example, we will use the `labeled_pairwise_string` evaluator from LangChain off-the-shelf. By default, instructs the evaluation llm to choose the preference based on helpfulness, relevance, correctness, and depth of thought. In your case, you will likely want to customize the criteria used!\n",
    "\n",
    "For more information on how to configure it, check out the [Labeled Pairwise String Evaluator](https://python.langchain.com/docs/guides/evaluation/comparison/labeled_pairwise_string) documentation and inspect the resulting traces when running this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2e1f12e-ed28-46ce-b09b-37088c475bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "pairwise_evaluator = load_evaluator(\"labeled_pairwise_string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f65e8572-18a3-46b3-936a-38e566b77922",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "\n",
    "eval_func = functools.partial(\n",
    "    predict_preference,\n",
    "    project_a=project_name,\n",
    "    project_b=project_name_2,\n",
    "    eval_chain=pairwise_evaluator,\n",
    ")\n",
    "\n",
    "\n",
    "# We will wrap in a lambda to take advantage of its default `batch` convenience method\n",
    "runnable = RunnableLambda(eval_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16969ec1-d27c-4f8e-9624-8c3f25c51f8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T04:45:43.550214Z",
     "start_time": "2023-09-20T04:45:43.459644Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_id=UUID('a0ceb51f-002d-4dd3-b4d6-bda9cb6bf958') inputs={'question': 'Why do I have to set environment variables?'} outputs={'answer': 'Environment variables can tell your LangChain application to perform tracing and contain the information necessary to authenticate to LangSmith. While there are other ways to connect, environment variables tend to be the simplest way to configure your application.'} id=UUID('960664ec-5777-46b9-a04d-1054c3530d19') created_at=datetime.datetime(2023, 9, 20, 21, 23, 16, 210654) modified_at=datetime.datetime(2023, 9, 20, 21, 23, 16, 363607) runs=[]dataset_id=UUID('a0ceb51f-002d-4dd3-b4d6-bda9cb6bf958') inputs={'question': 'How do I move my project between organizations?'} outputs={'answer': \"LangSmith doesn't directly support moving projects between organizations.\"} id=UUID('6f2c55a3-c17b-456e-9eed-aa74e2b5dcd5') created_at=datetime.datetime(2023, 9, 20, 21, 23, 16, 417243) modified_at=datetime.datetime(2023, 9, 20, 21, 23, 16, 562850) runs=[]\n",
      "\n",
      "dataset_id=UUID('a0ceb51f-002d-4dd3-b4d6-bda9cb6bf958') inputs={'question': 'Can I trace my Llama V2 llm?'} outputs={'answer': \"So long as you are using one of LangChain's LLM implementations, all your calls can be traced\"} id=UUID('ae14180c-180a-4782-a095-d6ab0b93e96b') created_at=datetime.datetime(2023, 9, 20, 21, 23, 16, 8755) modified_at=datetime.datetime(2023, 9, 20, 21, 23, 16, 156453) runs=[]\n",
      "dataset_id=UUID('a0ceb51f-002d-4dd3-b4d6-bda9cb6bf958') inputs={'question': 'How do I use a traceable decorator?'} outputs={'answer': 'The traceable decorator is available in the langsmith python SDK. To use, configure your environment with your API key,import the required function, decorate your function, and then call the function. Below is an example:\\n```python\\nfrom langsmith.run_helpers import traceable\\n@traceable(run_type=\"chain\") # or \"llm\", etc.\\ndef my_function(input_param):\\n    # Function logic goes here\\n    return output\\nresult = my_function(input_param)\\n```'} id=UUID('33c219df-bcd5-4dfd-a9d1-aad539751eab') created_at=datetime.datetime(2023, 9, 20, 21, 23, 15, 815990) modified_at=datetime.datetime(2023, 9, 20, 21, 23, 15, 950023) runs=[]\n",
      "dataset_id=UUID('a0ceb51f-002d-4dd3-b4d6-bda9cb6bf958') inputs={'question': \"What's a langsmith dataset?\"} outputs={'answer': 'A LangSmith dataset is a collection of examples. Each example contains inputs and optional expected outputs or references for that data point.'} id=UUID('c212f1c0-3d5c-42d6-9645-21a5b1e945c1') created_at=datetime.datetime(2023, 9, 20, 21, 23, 15, 615781) modified_at=datetime.datetime(2023, 9, 20, 21, 23, 15, 758555) runs=[]\n",
      "dataset_id=UUID('a0ceb51f-002d-4dd3-b4d6-bda9cb6bf958') inputs={'question': 'How might I query for all runs in a project?'} outputs={'answer': \"client.list_runs(project_name='my-project-name'), or in TypeScript, client.ListRuns({projectName: 'my-project-anme'})\"} id=UUID('1a3a74c9-9465-47a9-a684-3ed8e38b5fe4') created_at=datetime.datetime(2023, 9, 20, 21, 23, 15, 406083) modified_at=datetime.datetime(2023, 9, 20, 21, 23, 15, 548728) runs=[]\n",
      "dataset_id=UUID('a0ceb51f-002d-4dd3-b4d6-bda9cb6bf958') inputs={'question': 'What is LangChain?'} outputs={'answer': 'LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.'} id=UUID('24d31c8f-d829-4e07-b38c-907d68ed7bf2') created_at=datetime.datetime(2023, 9, 20, 21, 23, 15, 201014) modified_at=datetime.datetime(2023, 9, 20, 21, 23, 15, 344224) runs=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Invalid verdict: Final Decision: [[B. Verdict must be one of 'A', 'B', or 'C'.\n"
     ]
    }
   ],
   "source": [
    "examples = list(client.list_examples(dataset_id=dataset.id))\n",
    "values = runnable.batch(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be2783-6c15-4fb9-8fdc-11eaa91197fa",
   "metadata": {},
   "source": [
    "By running the function above, the \"preference\" feedback was automatically logged to the test projects you created in step 3. Below is a view of the same test run as before with the preference scores added. This model seems to be less preferred than the other! \n",
    "\n",
    "![Preference Tags](img/with_preferences.png)\n",
    "\n",
    "The `predict_preference` function we wrote above is set up to not log feedback in the case of a tie, meaning some of the examples do not have a corresponding preference score. You can adjust this behavior as you see fit. \n",
    "\n",
    "You can also view the feedback results for the other test run in the app to see how well the evaluator's results match your expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46c7d7f7-9e26-4439-8a2c-93f6d89741ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .hover_table td {\n",
       "            max-width: 200px; /* You can adjust this value */\n",
       "            overflow: hidden;\n",
       "            text-overflow: ellipsis;\n",
       "            white-space: nowrap;\n",
       "        }\n",
       "        .hover_table td:hover {\n",
       "            white-space: normal;\n",
       "            word-wrap: break-word;\n",
       "        }\n",
       "    </style>\n",
       "    <table border=\"1\" class=\"dataframe hover_table\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>input</th>\n",
       "      <th>answer</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>Preferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>How do I move my project between organizations?</td>\n",
       "      <td>LangSmith doesn't directly support moving projects between organizations.</td>\n",
       "      <td>Currently, LangSmith does not support project migration between organizations. If you want to move a project from one organization to another, the recommended approach is to manually export the runs and datasets from the source organization and import them into the destination organization.\\n\\nHere are the general steps to move a project between organizations:\\n\\n1. Export Runs: Use the LangSmith SDK or API to export the runs from the source organization. You can iterate through the runs and save them to a file or database.\\n\\n2. Export Datasets: If your project uses datasets, export them from the source organization using the LangSmith SDK or API.\\n\\n3. Import Runs: In the destination organization, create a new project and import the exported runs. Use the LangSmith SDK or API to create new runs in the destination project and populate them with the exported run data.\\n\\n4. Import Datasets: If your project uses datasets, import them into the destination organization using the LangSmith SDK or API.\\n\\nBy following these steps, you can effectively move your project from one organization to another. Remember to update any references or configurations in your application to use the new organization and project.\\n\\nIf you have a large number of runs or complex data dependencies, it may be helpful to reach out to the LangSmith support team at support@langchain.dev for further assistance and guidance.</td>\n",
       "      <td>Currently, LangSmith does not support project migration between organizations. However, you can manually export and import runs and datasets using the SDK to imitate project migration. Here's how you can do it:\\n\\n1. Export Runs: Use the LangSmith SDK to export the runs from your current organization. You can use the `export_run` function to export individual runs or the `export_runs` function to export multiple runs at once. Save the exported runs to a file or storage location.\\n\\n2. Export Datasets: Similarly, use the LangSmith SDK to export the datasets from your current organization. You can use the `export_dataset` function to export individual datasets or the `export_datasets` function to export multiple datasets at once. Save the exported datasets to a file or storage location.\\n\\n3. Create a New Project: In your new organization, create a new project where you want to import the runs and datasets.\\n\\n4. Import Runs: Use the LangSmith SDK to import the exported runs into the new project. You can use the `import_run` function to import individual runs or the `import_runs` function to import multiple runs at once. Provide the path or location of the exported runs file.\\n\\n5. Import Datasets: Similarly, use the LangSmith SDK to import the exported datasets into the new project. You can use the `import_dataset` function to import individual datasets or the `import_datasets` function to import multiple datasets at once. Provide the path or location of the exported datasets file.\\n\\nBy following these steps, you can effectively move your project from one organization to another.</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Why do I have to set environment variables?</td>\n",
       "      <td>Environment variables can tell your LangChain application to perform tracing and contain the information necessary to authenticate to LangSmith. While there are other ways to connect, environment variables tend to be the simplest way to configure your application.</td>\n",
       "      <td>Setting environment variables is necessary to configure the runtime environment for your LangSmith application. These variables provide important information such as the API key, project name, and endpoint to connect to LangSmith services.\\n\\nBy setting environment variables, you can easily manage and update these configurations without modifying your code. It also allows you to keep sensitive information, such as API keys, separate from your codebase, enhancing security.\\n\\nIn the context of LangSmith, setting the LANGCHAIN_TRACING_V2, LANGCHAIN_ENDPOINT, LANGCHAIN_API_KEY, and LANGCHAIN_PROJECT environment variables is essential for enabling tracing, specifying the API endpoint, and providing authentication credentials for your LangSmith project.\\n\\nOverall, environment variables provide a flexible and secure way to configure your LangSmith application without hardcoding sensitive information into your code.</td>\n",
       "      <td>Setting environment variables is a common practice in software development to configure the behavior of applications. In the context of LangSmith's tracing feature, setting environment variables is necessary to enable the logging of run traces to LangSmith.\\n\\nBy setting the `LANGCHAIN_TRACING_V2` environment variable, LangSmith's tracing feature is activated, and all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. This allows you to debug and analyze the behavior of your application, understand the inputs and outputs of LLM calls, trace the sequence of events in complicated chains and agents, monitor latency and token usage, and more.\\n\\nSetting environment variables provides a convenient and consistent way to enable tracing across different environments and programming languages. However, if your specific situation doesn't permit the use of environment variables or doesn't expose `process.env` (in the case of JavaScript runtime environments), there are alternative methods available to enable tracing without environment variables. These methods involve manually creating the `LangChainTracer` callback and passing it to the LangChain components during initialization or in the call itself.</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Can I trace my Llama V2 llm?</td>\n",
       "      <td>So long as you are using one of LangChain's LLM implementations, all your calls can be traced</td>\n",
       "      <td>Yes, you can trace your Llama V2 LLM using LangSmith. LangSmith's tracing feature allows you to log the inputs, outputs, and other relevant information of your LLM calls. This can be helpful for debugging and understanding the behavior of your LLM.\\n\\nTo enable tracing for your Llama V2 LLM, you need to set the `LANGCHAIN_TRACING_V2` environment variable to `true` before running your code. This can be done in your Python environment or in your shell.\\n\\nHere's an example of how to enable tracing for your Llama V2 LLM in Python:\\n\\n```python\\nimport os\\n\\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\\n\\n# Your Llama V2 LLM code here\\n```\\n\\nOnce tracing is enabled, all calls to your Llama V2 LLM will be logged to LangSmith. You can then use the LangSmith client to view and analyze the traces.\\n\\nNote that tracing is enabled at the environment level, so it will apply to all LLM calls made within that environment. If you want to selectively trace specific LLM calls, you can use the `LangChainTracer` callback and pass it to the LLM object during initialization or in the call itself.\\n\\n```python\\nfrom langchain.callbacks import LangChainTracer\\nfrom langchain.llm import LlamaV2LLM\\n\\nllm = LlamaV2LLM()\\nllm.invoke(\"Hello, world!\", config={\"callbacks\": [LangChainTracer()]})\\n```\\n\\nBy using the `LangChainTracer` callback, you can have more fine-grained control over which LLM calls are traced.\\n\\nRemember to handle your API key and other authentication details properly to ensure secure and authorized access to LangSmith.</td>\n",
       "      <td>Yes, you can trace your Llama V2 LLM by enabling tracing in your LangChain application. Tracing allows you to log the inputs and outputs of each component in your LLM application, including the Llama V2 LLM. This can be helpful for debugging and understanding the behavior of your LLM.\\n\\nTo enable tracing, you need to set the `LANGCHAIN_TRACING_V2` environment variable to `true` before running your LangChain application. This will ensure that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith.\\n\\nHere's an example of how to enable tracing in Python:\\n\\n```python\\nimport os\\nfrom langchain.chat_models import LlamaV2\\n\\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\\n\\nllm = LlamaV2()\\nllm.invoke(\"Hello, world!\")\\n```\\n\\nAnd here's an example in TypeScript:\\n\\n```typescript\\nimport { LlamaV2 } from \"langchain/chat_models/llama_v2\";\\n\\nprocess.env.LANGCHAIN_TRACING_V2 = \"true\";\\n\\nconst llm = new LlamaV2();\\nawait llm.invoke(\"Hello, world!\");\\n```\\n\\nRemember to replace `\"Hello, world!\"` with your desired input to the LLM.</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>How do I use a traceable decorator?</td>\n",
       "      <td>The traceable decorator is available in the langsmith python SDK. To use, configure your environment with your API key,import the required function, decorate your function, and then call the function. Below is an example:\\n```python\\nfrom langsmith.run_helpers import traceable\\n@traceable(run_type=\"chain\") # or \"llm\", etc.\\ndef my_function(input_param):\\n    # Function logic goes here\\n    return output\\nresult = my_function(input_param)\\n```</td>\n",
       "      <td>To use the `traceable` decorator in LangSmith, you can follow these steps:\\n\\n1. Import the necessary modules and functions:\\n\\n```python\\nimport datetime\\nfrom typing import Any\\n\\nimport openai\\nfrom langsmith.run_helpers import traceable\\n```\\n\\n2. Define your functions or methods that you want to trace. Decorate them with the `traceable` decorator, specifying the `run_type` and `name` parameters:\\n\\n```python\\n@traceable(run_type=\"llm\", name=\"openai.ChatCompletion.create\")\\ndef my_llm(*args: Any, **kwargs: Any) -&gt; dict:\\n    return openai.ChatCompletion.create(*args, **kwargs)\\n\\n@traceable(run_type=\"tool\")\\ndef my_tool(tool_input: str) -&gt; str:\\n    return tool_input.upper()\\n\\n@traceable(run_type=\"chain\")\\ndef my_chain(prompt: str) -&gt; str:\\n    messages = [\\n        {\\n            \"role\": \"system\",\\n            \"content\": \"You are an AI Assistant. The time is \" + str(datetime.datetime.now()),\\n        },\\n        {\"role\": \"user\", \"content\": prompt},\\n    ]\\n    return my_llm(model=\"gpt-3.5-turbo\", messages=messages)\\n\\n@traceable(run_type=\"chain\")\\ndef my_chat_bot(text: str) -&gt; str:\\n    generated = my_chain(text)\\n\\n    if \"meeting\" in generated:\\n        return my_tool(generated)\\n    else:\\n        return generated\\n```\\n\\n3. Call your traced functions as usual. The calls will be automatically traced and logged:\\n\\n```python\\nmy_chat_bot(\"Summarize this morning's meetings.\")\\n```\\n\\nYou can view the example run and the call hierarchy in the LangSmith app by visiting the provided URL.\\n\\nNote: The `traceable` decorator is used to trace the function calls and generate a run tree. It does not modify the behavior of the functions themselves.</td>\n",
       "      <td>The `traceable` decorator is a convenient way to log your function calls as runs in LangSmith. It automatically captures the inputs, outputs, and metadata of the function and sends them to LangSmith for logging.\\n\\nTo use the `traceable` decorator, follow these steps:\\n\\n1. Import the `traceable` decorator from the `langsmith.run_helpers` module.\\n2. Decorate your function with `@traceable(run_type=\"chain\")`, specifying the `run_type` as \"chain\" (or any other appropriate value).\\n3. Call your decorated function as usual.\\n\\nHere's an example in Python:\\n\\n```python\\nfrom langsmith.run_helpers import traceable\\n\\n@traceable(run_type=\"chain\")\\ndef my_function(input1, input2):\\n    # Your function logic here\\n    output = input1 + input2\\n    return output\\n\\nresult = my_function(3, 4)\\n```\\n\\nIn this example, the `my_function` is decorated with `@traceable(run_type=\"chain\")`. When the function is called, the inputs, outputs, and metadata will be logged as a run in LangSmith.\\n\\nNote that you need to have the LangSmith SDK installed and configured with your API key and project name for the `traceable` decorator to work properly.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What's a langsmith dataset?</td>\n",
       "      <td>A LangSmith dataset is a collection of examples. Each example contains inputs and optional expected outputs or references for that data point.</td>\n",
       "      <td>A LangSmith dataset is a collection of input-output examples that are used for testing and evaluating language models, chains, and agents. It is a structured set of data that represents different scenarios and expected outputs.\\n\\nIn LangSmith, you can create datasets by adding examples from your application runs. Each example consists of an input and the corresponding expected output. These examples can be added at different steps of a nested chain, including end-to-end chains, intermediary chains (such as LLM chains), or individual LLM or Chat Model calls.\\n\\nBy collecting examples in a dataset, you can create a benchmark for your models and chains. This allows you to test and evaluate the performance of your application against known scenarios and expected outputs. Datasets are particularly useful for comparing different versions of your chains and models, as well as for identifying and addressing issues or failures in your application.\\n\\nLangSmith provides features to easily add examples to datasets, edit examples before adding them, and visualize the dataset for analysis and evaluation.</td>\n",
       "      <td>A LangSmith dataset refers to a collection of data that is used for training, testing, or evaluating language models within the LangSmith framework. Datasets in LangSmith typically consist of text data that is used to train language models or evaluate their performance.\\n\\nLangSmith provides a variety of built-in datasets that you can use, such as text classification datasets, question-answering datasets, sentiment analysis datasets, and more. These datasets are designed to be easily accessible and compatible with the LangSmith framework.\\n\\nIn addition to the built-in datasets, you can also create your own custom datasets in LangSmith. This allows you to tailor the data to your specific application or use case.\\n\\nOverall, LangSmith datasets play a crucial role in training and evaluating language models, enabling you to build powerful and accurate language-based applications.</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>How might I query for all runs in a project?</td>\n",
       "      <td>client.list_runs(project_name='my-project-name'), or in TypeScript, client.ListRuns({projectName: 'my-project-anme'})</td>\n",
       "      <td>To query for all runs in a project, you can use the LangSmith client's `list_runs` method. Here's an example in Python:\\n\\n```python\\nfrom langsmith import Client\\n\\nclient = Client()\\nruns = list(client.list_runs(project_name=\"&lt;your_project&gt;\"))\\n```\\n\\nAnd here's an example in TypeScript:\\n\\n```typescript\\nimport { Client, Run } from \"langsmith\";\\n\\nconst client = new Client();\\nconst runs: Run[] = [];\\nfor await (const run of client.listRuns({ projectName: \"&lt;your_project&gt;\" })) {\\n  runs.push(run);\\n}\\n```\\n\\nIn both examples, replace `&lt;your_project&gt;` with the name of your project. This will retrieve all runs in the specified project.</td>\n",
       "      <td>To query for all runs in a project using the LangSmith SDK, you can use the `listRuns` method of the `Client` class. Here's an example in Python:\\n\\n```python\\nfrom langsmith import Client\\n\\nclient = Client()\\nruns = list(client.list_runs(project_name=\"&lt;your_project&gt;\"))\\n```\\n\\nAnd here's an example in TypeScript:\\n\\n```typescript\\nimport { Client, Run } from \"langsmith\";\\n\\nconst client = new Client();\\nconst runs: Run[] = [];\\nfor await (const run of client.listRuns({ projectName: \"&lt;your_project&gt;\" })) {\\n  runs.push(run);\\n}\\n```\\n\\nIn both examples, replace `&lt;your_project&gt;` with the name of your project. This will retrieve all runs in the specified project.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What is LangChain?</td>\n",
       "      <td>LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.</td>\n",
       "      <td>LangChain is an open-source framework for building applications using large language models (LLMs). It provides a set of tools and libraries that simplify the development, testing, and evaluation of LLM-based applications. LangChain seamlessly integrates with various LLM frameworks, allowing developers to leverage the power of LLMs in their applications.\\n\\nWith LangChain, you can easily create LLM chains, which are sequences of LLM calls that work together to achieve a specific task. You can also build intelligent agents that interact with users and make use of LLMs for natural language understanding and generation. LangChain provides features for debugging, testing, and monitoring LLM applications, making it easier to identify and fix issues.\\n\\nLangChain is developed by LangChain, the company behind LangSmith, a platform for building production-grade LLM applications. LangSmith provides additional features and capabilities for debugging, testing, and evaluating LLM applications built with LangChain.</td>\n",
       "      <td>LangChain is a framework for building applications using large language models. It provides a set of tools and libraries that make it easier to interact with language models and integrate them into your applications. With LangChain, you can leverage the power of language models to perform tasks such as text generation, translation, summarization, and more. LangChain supports various programming languages and provides an API for seamless integration with your applications.</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.DataFrame(values)\n",
    "word_wrap_on_hover(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ec77a-67b3-48e8-9cde-c9022641f245",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this walkthrough, you compared two versions of a RAG Q&A chain by predicting preference scores for each pair of predictions.\n",
    "This approach is one way to automatically compare two versions of a chain that can give additional context beyond regular benchmarking.\n",
    "\n",
    "There are many related ways to evaluate preferences! Here, we used binary choices to compare the two models and only evaluated once, but you may get better results by trying one of the following approaches:\n",
    "\n",
    "- Evaluate multiple times in each position and returning a win rate\n",
    "- Ensemble evaluators\n",
    "- Instruct the model to output continuous scores\n",
    "- Instruct the model to use a different prompt strategy than chain of thought\n",
    "\n",
    "For more information on measuring the reliability of this and other approaches, you can check out the [evaluations examples](https://python.langchain.com/docs/guides/evaluation/examples) in the LangChain repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73b5ac9-1fd1-4eaa-bc6d-53263cd575bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
