{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a7184af-d54f-487d-ad7f-0f3274dc689b",
   "metadata": {},
   "source": [
    "# Evaluating Using Fixed Sources\n",
    "\n",
    "A simple RAG pipeline is composed of (at least) two components: the retriever and the response generator. While you can evaluate the whole chain end-to-end, as shown in the [QA Correctness](../qa-correctness/) walkthrough, but you likely will get more actionable and fine-grained metrics by evaluating each component in isolation.\n",
    "\n",
    "This example addresses evaluating the response generation component by fixing the retrieved documents within the dataset inputs. In this walkthrough, you will use this dataset to evaluate the response generator using both correctenss and a custom \"faithfulness\" evaluator.\n",
    "\n",
    "![Custom Evaluator](./img/example_results.png)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "First, install the required packages and configure your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c748b92-e590-408f-bd20-733dc79d643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain openai anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c086f0-f1c4-4a55-a922-c926239de2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Update with your API URL if using a hosted instance of Langsmith.\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\" # Update with your API key\n",
    "uid = uuid.uuid4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a0309-48f8-4770-8b34-2b97eb85a247",
   "metadata": {},
   "source": [
    "## 1. Create a dataset\n",
    "\n",
    "Next, we'll create a dataset. This simple dataset below will illustrate the correctness and faithfulness metrics we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f83f2e-76d1-4d86-9275-35bd61df014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple example dataset\n",
    "examples = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"question\": \"What's the company's total revenue for q2 of 2022?\",\n",
    "            \"documents\": [\n",
    "                {\n",
    "                    \"metadata\": {},\n",
    "                    \"page_content\": \"In q1 the lemonade company made $4.95. In q2 revenue increased by a sizeable amount to just over $2T dollars.\"\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"label\": \"2 trillion dollars\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"question\": \"Who is Lebron?\",\n",
    "            \"documents\": [\n",
    "                {\n",
    "                    \"metadata\": {},\n",
    "                    \"page_content\": \"On Thursday, February 16, Lebron James was nominated as President of the United States.\"\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"label\": \"Lebron James is the President of the USA.\",\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adf85a45-e100-4d28-a102-ec1d135f0ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = f\"Faithfulness Example - {uid}\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51f9bb75-c5ff-4277-bb1f-b969d2f68675",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_examples(inputs=[e[\"inputs\"] for e in examples], outputs=[e[\"outputs\"] for e in examples], dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa0264-d24f-495a-b9f6-87ddf97aaeb6",
   "metadata": {},
   "source": [
    "## 2. Define Chain\n",
    "\n",
    "Suppose your chain is composed of two main components: a retriever and response synthesizer. Using LangChain runnables, it's easy to separate these two components to evaluate them in isolation.\n",
    "\n",
    "Below is a very simple RAG chain with a placeholder retriever. For our testing, we will evaluate ONLY the response synthesizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6314168f-9530-476f-949b-d49c40db55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import chat_models, prompts\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.retriever import BaseRetriever, Document\n",
    "\n",
    "class MyRetriever(BaseRetriever):\n",
    "    def _get_relevant_documents(self, query, *, run_manager):\n",
    "        return [Document(page_content=\"Example\")]\n",
    "\n",
    "# This is what we will evaluate\n",
    "response_synthesizer = (\n",
    "    prompts.ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"Respond using the following documents as context:\\n{documents}\"),\n",
    "            (\"user\", \"{question}\")\n",
    "        ]\n",
    "    ) | chat_models.ChatAnthropic(model=\"claude-2\", max_tokens=1000)\n",
    ")\n",
    "\n",
    "# Full chain below for illustration\n",
    "chain = (\n",
    "    {\n",
    "        \"documents\": MyRetriever(),\n",
    "        \"qusetion\": RunnablePassthrough(),\n",
    "    }\n",
    "    | response_synthesizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6087fa-432d-4a59-b023-29058b5ec6ea",
   "metadata": {},
   "source": [
    "## 3. Evaluate\n",
    "\n",
    "Below, we will define a custom \"FaithfulnessEvaluator\" that measures how faithful the chain's output prediction is to the reference input documents, given the user's input question.\n",
    "\n",
    "In this case, we will wrap the [Scoring Eval Chain](https://python.langchain.com/docs/guides/evaluation/string/scoring_eval_chain) and manually select which fields in the run and dataset example to use to represent the prediction, input question, and reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8217e940-e6d0-4f08-bed7-41cda7a35ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import RunEvaluator, EvaluationResult\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "class FaithfulnessEvaluator(RunEvaluator):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.evaluator = load_evaluator(\n",
    "            \"labeled_score_string\", \n",
    "            criteria={\"faithful\": \"How faithful is the submission to the reference context?\"},\n",
    "            normalize_by=10,\n",
    "        )\n",
    "\n",
    "    def evaluate_run(self, run, example) -> EvaluationResult:\n",
    "        res = self.evaluator.evaluate_strings(\n",
    "            prediction=next(iter(run.outputs.values())),\n",
    "            input=run.inputs[\"question\"],\n",
    "            # We are treating the documents as the reference context in this case.\n",
    "            reference=example.inputs[\"documents\"],\n",
    "        )\n",
    "        return EvaluationResult(key=\"labeled_criteria:faithful\", **res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70e56fe8-ae02-481d-b6f6-729e535c5e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-spotless-vessel-38' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/79c76eb3-d2a6-4274-ab35-d64efeb6ec20\n",
      "[------------------------------------------------->] 2/2"
     ]
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"qa\"],\n",
    "    custom_evaluators=[FaithfulnessEvaluator()],\n",
    "    input_key=\"question\",\n",
    ")\n",
    "results = client.run_on_dataset(\n",
    "    llm_or_chain_factory=response_synthesizer,\n",
    "    dataset_name=dataset_name,\n",
    "    evaluation=eval_config,\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e74b5b-b8e2-433d-a9da-c56539152833",
   "metadata": {},
   "source": [
    "You can review the results in LangSmith to see how the chain fares. The trace for the custom faithfulness evaluator should look something like this:\n",
    "\n",
    "[![](./img/example_score.png)](https://smith.langchain.com/public/9a4e6ee2-f26c-4bcd-a050-04766fbfd350/r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9286b-0463-4f81-a27b-b2e3b16955c2",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "\n",
    "Most of LangChain's open-source evaluators implement the \"[StringEvaluator](https://python.langchain.com/docs/guides/evaluation/string/)\" interface, meaning they compute a metric based on:\n",
    "\n",
    "- An input string from the dataset example inputs (configurable by the RunEvalConfig's input_key property)\n",
    "- An output prediction string from the evaluated chain's outputs (configurable by the RunEvalConfig's prediction_key property)\n",
    "- (If labels are required) a reference string from the example outputs (configurable by the RunEvalConfig's reference_key property)\n",
    "\n",
    "However, you may want to compute metrics based on multiple fields in the dataset. For instance, you could simulataneously compute standard labeled metrics like \"correctness\" as well as other metrics like \"faithfulness\" according to some additional input fields. It's easy to measure your chain's performance using data from various fields in the example inputs and outputs by configuring a custom RunEvaluator.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
