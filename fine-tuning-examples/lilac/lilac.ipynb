{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Curate fine-tuning data with Lilac\n",
    "\n",
    "Lilac is an open-source product that helps you analyze, structure, and clean unstructured data with AI. You can use it to enrich datasets of LangChain runs to create better fine-tuning datasets.\n",
    "\n",
    "In this walkthrough, we will use Lilac on a dataset of LangSmith runs to check for PII and remove approximate duplicates.\n",
    "\n",
    "The basic workflow is as follows:\n",
    "\n",
    "- Create a LangSmith dataset of runs data.\n",
    "- Load LangSmith dataset into Lilac.\n",
    "- Filter and curate dataset using signals and concepts.\n",
    "- Export the dataset for fine-tuning.\n",
    "\n",
    "We will explain each of these steps in more detail below, but first, install some prerequisite packages.\n",
    "\n",
    "## Setup\n",
    "\n",
    "In addition to Lilac and LangSmith, this walkthrough requires a couple of additional packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"lilac[pii]\" langdetect openai langchain --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import os\n",
    "\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"<YOUR-API-KEY>\"\n",
    "unique_id = uuid.uuid4().hex[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Create LangSmith dataset\n",
    "\n",
    "We've included an example dataset in this repository that you can use to complete this walkthrough.\n",
    "\n",
    "This dataset was made by querying prompt and LLM runs from an example deployment of [chat langchain](https://github.com/langchain-ai/chat-langchain). \n",
    "\n",
    "For more information on how to query runs in LangSmith, check out the [docs](https://docs.smith.langchain.com/tracing/use-cases/export-runs/local) or explore some of the other recipes in this cookbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = f\"langsmith-prompt-runs-{unique_id}\"\n",
    "ds = client.create_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def create_example(line: str):\n",
    "    d = json.loads(line)\n",
    "    client.create_example(inputs=d['inputs'], outputs=d['outputs'], dataset_id=ds.id)\n",
    "\n",
    "with open('rag.jsonl', 'r', encoding='utf-8') as f:\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        executor.map(\n",
    "               create_example, \n",
    "            f\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can create the dataset. Lilac works best on flat dataset structures, so we will flatten (and stringify) some of the attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import into Lilac\n",
    "\n",
    "Next, we can import the LangSmith dataset into Lilac. Select the dataset name you created above, \n",
    "and run the code below. Once you've run the code, you can view the the results in Lilac's UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import lilac as ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll.set_project_dir('./langsmith-finetune')\n",
    "\n",
    "data_source = ll.sources.langsmith.LangSmithSource(\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "config = ll.DatasetConfig(\n",
    "  namespace='local',\n",
    "  name=dataset_name,\n",
    "  source=data_source,\n",
    ")\n",
    "\n",
    "dataset = ll.create_dataset(config)\n",
    "ll.start_server()\n",
    "# await ll.stop_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Enrich Dataset\n",
    "\n",
    "Now that we have our dataset in Lilac, we can run Lilac’s signals, concepts and labels to help organize and filter the dataset. Our goal is to select distinct examples demonstrating good language model generations for a variety of input types. You can explore and annotate the dataset in the app by navigating to the URL printed out by the local server above. I'd encourage you to try out their off-the-shelf \"concepts\" or try training your own.\n",
    "\n",
    "For the sake of this walkthrough, we will focus on using the Python API. You can follow along with the code below.\n",
    "\n",
    "#### Applying 'signals'\n",
    "\n",
    "Signals in Lilac refer to any function that is applied over a field. We will use a couple off-the-shelf \"signals\" to perform the following:\n",
    "\n",
    "- PII detection: we don't want to leak private data\n",
    "- Near duplicate detection: we want each training example to be informative\n",
    "\n",
    "These are useful for filtering bad examples from our dataset before fine-tuning a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing pii on local/langsmith-prompt-runs-e8ae0676:('question',): 100%|████████████████████████████████████████████████████████████████████████████████████| 369/369 [00:00<00:00, 793.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing signal \"pii\" on local/langsmith-prompt-runs-e8ae0676:('question',) took 0.467s.\n",
      "Wrote signal output to ./langsmith-finetune/datasets/local/langsmith-prompt-runs-e8ae0676/question/pii\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing pii on local/langsmith-prompt-runs-e8ae0676:('output',): 100%|██████████████████████████████████████████████████████████████████████████████████████| 369/369 [00:00<00:00, 418.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing signal \"pii\" on local/langsmith-prompt-runs-e8ae0676:('output',) took 0.883s.\n",
      "Wrote signal output to ./langsmith-finetune/datasets/local/langsmith-prompt-runs-e8ae0676/output/pii\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing near_dup on local/langsmith-prompt-runs-e8ae0676:('question',):   0%|                                                                                          | 0/369 [00:00<?, ?it/s]\n",
      "Fingerprinting...: 369it [00:00, 16802.53it/s]\n",
      "\n",
      "Computing hash collisions...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 908.25it/s]\u001b[A\n",
      "\n",
      "Clustering...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 29018.60it/s]\u001b[A\n",
      "Computing near_dup on local/langsmith-prompt-runs-e8ae0676:('question',): 100%|██████████████████████████████████████████████████████████████████████████████| 369/369 [00:00<00:00, 5708.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing signal \"near_dup\" on local/langsmith-prompt-runs-e8ae0676:('question',) took 0.066s.\n",
      "Wrote signal output to ./langsmith-finetune/datasets/local/langsmith-prompt-runs-e8ae0676/question/near_dup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing near_dup on local/langsmith-prompt-runs-e8ae0676:('output',):   0%|                                                                                            | 0/369 [00:00<?, ?it/s]\n",
      "Fingerprinting...: 361it [00:00, 3975.29it/s]\n",
      "\n",
      "Computing hash collisions...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 924.87it/s]\u001b[A\n",
      "\n",
      "Clustering...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 46643.24it/s]\u001b[A\n",
      "Computing near_dup on local/langsmith-prompt-runs-e8ae0676:('output',): 100%|████████████████████████████████████████████████████████████████████████████████| 369/369 [00:00<00:00, 2819.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing signal \"near_dup\" on local/langsmith-prompt-runs-e8ae0676:('output',) took 0.132s.\n",
      "Wrote signal output to ./langsmith-finetune/datasets/local/langsmith-prompt-runs-e8ae0676/output/near_dup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.compute_signal(ll.PIISignal(), 'question')\n",
    "dataset.compute_signal(ll.PIISignal(), 'output')\n",
    "\n",
    "# Apply min-hash LSH (https://en.wikipedia.org/wiki/MinHash) to detect approximate n-gram duplicates\n",
    "dataset.compute_signal(ll.NearDuplicateSignal(), 'question')\n",
    "dataset.compute_signal(ll.NearDuplicateSignal(), 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding labels\n",
    "\n",
    "Labeling is best done in-app, but you can also programmatically [label rows using the python SDK](https://lilacml.com/datasets/dataset_labels.html). Below is an example that labels all rows not tagged as English as `not_english`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing lang_detection on local/langsmith-prompt-runs-e8ae0676:('question',): 100%|█████████████████████████████████████████████████████████████████████████| 369/369 [00:00<00:00, 818.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing signal \"lang_detection\" on local/langsmith-prompt-runs-e8ae0676:('question',) took 0.460s.\n",
      "Wrote signal output to ./langsmith-finetune/datasets/local/langsmith-prompt-runs-e8ae0676/question/lang_detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing lang_detection on local/langsmith-prompt-runs-e8ae0676:('output',): 100%|███████████████████████████████████████████████████████████████████████████| 369/369 [00:00<00:00, 498.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing signal \"lang_detection\" on local/langsmith-prompt-runs-e8ae0676:('output',) took 0.741s.\n",
      "Wrote signal output to ./langsmith-finetune/datasets/local/langsmith-prompt-runs-e8ae0676/output/lang_detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.compute_signal(ll.LangDetectionSignal(), 'question')\n",
    "dataset.compute_signal(ll.LangDetectionSignal(), 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check the current schema by running the following. Select the fields you want to export.\n",
    "# dataset.manifest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.add_labels(\n",
    "  'not_english',\n",
    "  filters=[\n",
    "    (('question', 'lang_detection'), 'not_equal', 'en')\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lilac has a lot more powerful capabilities like custom concepts and signals that you can apply. Check out their [docs](https://lilacml.com/blog/introducing-lilac.html) for more info, and see our [exploratory data analysis](../../exploratory-data-analysis/lilac/lilac.ipynb) noteboook for an introduction on using them with LangSmith datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare the enriched dataset\n",
    "\n",
    "Now let's prepare the dataset for fine-tuning, we will fetch the deduplicated rows and filter out any rows that may contain PII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 369\n",
      "Filtered length: 312\n"
     ]
    }
   ],
   "source": [
    "df = dataset.to_pandas([\n",
    "    'question', \n",
    "    'chat_history',\n",
    "    'context',\n",
    "    'output', \n",
    "    'question.pii',\n",
    "    'question.near_dup',\n",
    "    'user_score',\n",
    "    'not_english'])\n",
    "\n",
    "print(f\"Original length: {len(df)}\")\n",
    "\n",
    "# Flatten the dataframe\n",
    "df['cluster_id'] = df['question.near_dup'].apply(lambda x: x['cluster_id'])\n",
    "df['contains_pii'] = df['question.pii'].apply(lambda x: bool([v for l in x.values() for v in l]))\n",
    "df['not_english'] = df['not_english'].apply(lambda x: x is not None and x.get('label') == 'true')\n",
    "# Drop original dotted columns\n",
    "df.drop(columns=['question.near_dup', 'question.pii'], inplace=True)\n",
    "# Now filter for only rows for which contains_pii is false, user_score is 1.0\n",
    "df = df[(~df['contains_pii']) & (df['user_score'] != '0.0') & (~df['output'].isna())]\n",
    "# And drop the duplicate cluster IDs\n",
    "df = df.drop_duplicates(subset='cluster_id', keep='first')\n",
    "print(f\"Filtered length: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>chat_history</th>\n",
       "      <th>context</th>\n",
       "      <th>output</th>\n",
       "      <th>user_score</th>\n",
       "      <th>not_english</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>contains_pii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"index = create_index(PERSIST=False)   # def c...</td>\n",
       "      <td>[{\"content\": \"what StuffDocumentsChain does ? ...</td>\n",
       "      <td>\"&lt;doc id='0'&gt;# Tic-Tac-Toe\\n                \\n...</td>\n",
       "      <td>\"To fix the code and introduce the suggested c...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"ValidationError: 1 validation error for AIMes...</td>\n",
       "      <td>[{\"content\": \"can you add something to this co...</td>\n",
       "      <td>\"&lt;doc id='0'&gt;custom_message_converter=CustomMe...</td>\n",
       "      <td>\"AIMessage | \\ud83e\\udd9c\\ufe0f\\ud83d\\udd17 La...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"What is python\"</td>\n",
       "      <td>[{\"content\": \"alo\", \"example\": false, \"additio...</td>\n",
       "      <td>\"&lt;doc id='0'&gt;Python | \\ud83e\\udd9c\\ufe0f\\ud83d...</td>\n",
       "      <td>\"Python is a high-level programming language t...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\". Use 'basic_auth' or 'bearer_auth' parameter...</td>\n",
       "      <td>[{\"content\": \"I am new to python programming. ...</td>\n",
       "      <td>\"&lt;doc id='0'&gt;with search_distanceConversationa...</td>\n",
       "      <td>\"I apologize for the confusion. It seems there...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"test\"</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"metadata\": {\"source\": \"https://smith.langch...</td>\n",
       "      <td>{\"content\": \"Hello! How can I assist you with ...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>\"so in order to work with pdf and chatgpt that...</td>\n",
       "      <td>[{\"content\": \"i need to build a pdf database w...</td>\n",
       "      <td>\"&lt;doc id='0'&gt;With the EmbeddingsRedundantFilte...</td>\n",
       "      <td>\"Embeddings | \\ud83e\\udd9c\\ufe0f\\ud83d\\udd17 L...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>363</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>\"what is the answer to the universe?\"</td>\n",
       "      <td>[{\"content\": \"hello\", \"example\": false, \"addit...</td>\n",
       "      <td>[{\"metadata\": {\"source\": \"https://python.langc...</td>\n",
       "      <td>\"The answer to the universe is often humorousl...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>364</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>\"what if sqlalchemy is being used in SQLDataba...</td>\n",
       "      <td>[{\"content\": \"File \\\"/usr/local/lib/python3.11...</td>\n",
       "      <td>\"&lt;doc id='0'&gt;Querying a SQL DB | \\ud83e\\udd9c\\...</td>\n",
       "      <td>\"Hmm, I'm not sure about the specific implemen...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>365</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>\"can tools work with LLM Chain\"</td>\n",
       "      <td>[]</td>\n",
       "      <td>\"&lt;doc id='0'&gt;hardware and scaling independentl...</td>\n",
       "      <td>\"Yes, tools can work with LLM (Large Language ...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>366</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>\"Is it basically the same as using an external...</td>\n",
       "      <td>[{\"content\": \"for initialize_agent:\\n```\\nPara...</td>\n",
       "      <td>[{\"metadata\": {\"source\": \"https://python.langc...</td>\n",
       "      <td>\"Yes, the `AgentExecutor` returned by `initial...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>368</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0    \"index = create_index(PERSIST=False)   # def c...   \n",
       "1    \"ValidationError: 1 validation error for AIMes...   \n",
       "2                                     \"What is python\"   \n",
       "4    \". Use 'basic_auth' or 'bearer_auth' parameter...   \n",
       "5                                               \"test\"   \n",
       "..                                                 ...   \n",
       "363  \"so in order to work with pdf and chatgpt that...   \n",
       "364              \"what is the answer to the universe?\"   \n",
       "365  \"what if sqlalchemy is being used in SQLDataba...   \n",
       "366                    \"can tools work with LLM Chain\"   \n",
       "368  \"Is it basically the same as using an external...   \n",
       "\n",
       "                                          chat_history  \\\n",
       "0    [{\"content\": \"what StuffDocumentsChain does ? ...   \n",
       "1    [{\"content\": \"can you add something to this co...   \n",
       "2    [{\"content\": \"alo\", \"example\": false, \"additio...   \n",
       "4    [{\"content\": \"I am new to python programming. ...   \n",
       "5                                                   []   \n",
       "..                                                 ...   \n",
       "363  [{\"content\": \"i need to build a pdf database w...   \n",
       "364  [{\"content\": \"hello\", \"example\": false, \"addit...   \n",
       "365  [{\"content\": \"File \\\"/usr/local/lib/python3.11...   \n",
       "366                                                 []   \n",
       "368  [{\"content\": \"for initialize_agent:\\n```\\nPara...   \n",
       "\n",
       "                                               context  \\\n",
       "0    \"<doc id='0'># Tic-Tac-Toe\\n                \\n...   \n",
       "1    \"<doc id='0'>custom_message_converter=CustomMe...   \n",
       "2    \"<doc id='0'>Python | \\ud83e\\udd9c\\ufe0f\\ud83d...   \n",
       "4    \"<doc id='0'>with search_distanceConversationa...   \n",
       "5    [{\"metadata\": {\"source\": \"https://smith.langch...   \n",
       "..                                                 ...   \n",
       "363  \"<doc id='0'>With the EmbeddingsRedundantFilte...   \n",
       "364  [{\"metadata\": {\"source\": \"https://python.langc...   \n",
       "365  \"<doc id='0'>Querying a SQL DB | \\ud83e\\udd9c\\...   \n",
       "366  \"<doc id='0'>hardware and scaling independentl...   \n",
       "368  [{\"metadata\": {\"source\": \"https://python.langc...   \n",
       "\n",
       "                                                output user_score  \\\n",
       "0    \"To fix the code and introduce the suggested c...       None   \n",
       "1    \"AIMessage | \\ud83e\\udd9c\\ufe0f\\ud83d\\udd17 La...       None   \n",
       "2    \"Python is a high-level programming language t...       None   \n",
       "4    \"I apologize for the confusion. It seems there...       None   \n",
       "5    {\"content\": \"Hello! How can I assist you with ...       None   \n",
       "..                                                 ...        ...   \n",
       "363  \"Embeddings | \\ud83e\\udd9c\\ufe0f\\ud83d\\udd17 L...       None   \n",
       "364  \"The answer to the universe is often humorousl...       None   \n",
       "365  \"Hmm, I'm not sure about the specific implemen...       None   \n",
       "366  \"Yes, tools can work with LLM (Large Language ...       None   \n",
       "368  \"Yes, the `AgentExecutor` returned by `initial...       None   \n",
       "\n",
       "     not_english  cluster_id  contains_pii  \n",
       "0          False           0         False  \n",
       "1          False           1         False  \n",
       "2           True           2         False  \n",
       "4          False           4         False  \n",
       "5           True           5         False  \n",
       "..           ...         ...           ...  \n",
       "363        False         363         False  \n",
       "364         True         364         False  \n",
       "365        False         365         False  \n",
       "366         True         366         False  \n",
       "368        False         368         False  \n",
       "\n",
       "[312 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Finetune\n",
    "\n",
    "With the dataset filtered, we can now prepare it to a compatible format for fine-tuning.\n",
    "We will use OpenAI's fine-tuning endpoint for this, but you could also apply similar logic to finetune a Llama, T5, or other model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_messages(row):\n",
    "    # print(row)\n",
    "    chat_history = json.loads(row.chat_history) or []\n",
    "    roles = (\"assistant\", \"user\")\n",
    "    messages = [{\"role\": \"system\", \"content\": \"Helpfully answer the questions about LangChain.\"}]\n",
    "    for i, msg in enumerate(chat_history):\n",
    "        messages.append(\n",
    "            {\"role\": roles[i%2], \"content\": str(msg[\"content\"])}\n",
    "            )\n",
    "    messages.append({\"role\": \"user\", \"content\": row.question})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": row.output})\n",
    "    return messages\n",
    "\n",
    "messages = df.apply(create_messages, axis=1).tolist()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can fine-tune the model! This will take a while (20+ minutes), so we'd encourage you to further explore your local Lilac dataset\n",
    "while you wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status=[uploaded]... 0.00s\r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from io import BytesIO\n",
    "import time\n",
    "\n",
    "import openai\n",
    "\n",
    "# We will write the jsonl file in memory\n",
    "my_file = BytesIO()\n",
    "for m in messages:\n",
    "    my_file.write((json.dumps({\"messages\": m}) + \"\\n\").encode('utf-8'))\n",
    "\n",
    "my_file.seek(0)\n",
    "training_file = openai.File.create(\n",
    "  file=my_file,\n",
    "  purpose='fine-tune'\n",
    ")\n",
    "\n",
    "# OpenAI audits each training file for compliance reasons.\n",
    "# This make take a few minutes\n",
    "status = openai.File.retrieve(training_file.id).status\n",
    "start_time = time.time()\n",
    "while status != \"processed\":\n",
    "    print(f\"Status=[{status}]... {time.time() - start_time:.2f}s\", end=\"\\r\", flush=True)\n",
    "    time.sleep(5)\n",
    "    status = openai.File.retrieve(training_file.id).status\n",
    "print(f\"File {training_file.id} ready after {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "job = openai.FineTuningJob.create(\n",
    "    training_file=training_file.id,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "status = openai.FineTuningJob.retrieve(job.id).status\n",
    "start_time = time.time()\n",
    "while status != \"succeeded\":\n",
    "    print(f\"Status=[{status}]... {time.time() - start_time:.2f}s\", end=\"\\r\", flush=True)\n",
    "    time.sleep(5)\n",
    "    job = openai.FineTuningJob.retrieve(job.id)\n",
    "    status = job.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use fine-tuned model\n",
    "\n",
    "With the model fine-tuning complete, you can load the fine-tuned model directly in LangChain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=job.fine_tuned_model,\n",
    "    temperature=1,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Helpfully answer the questions about LangChain.\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "chain = prompt | model\n",
    "chain.invoke({\"input\": \"What's LangChain Expression Language?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "LangSmith makes it easy to collect unstructured data seen by your production LLM application. Lilac can make it easier to filter and analyze with sophisticated methods.\n",
    "\n",
    "In this tutorial you created a dataset of run traces, filtered by near-duplicates and looking for PII, then used the filtered dataset to fine-tune a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
