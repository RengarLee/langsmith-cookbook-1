{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b8fc9e-1346-4c7a-be2e-f88f092f5fd5",
   "metadata": {},
   "source": [
    "# LangSmith Tutorial (No LangChain)\n",
    "\n",
    "Here, we'll do a very basic walkthrough of how you can get started with LangSmith in TypeScript without LangChain.\n",
    "\n",
    "First, we'll do some setup. Create a LangSmith API Key by navigating to the settings page in LangSmith, then create an .env file with values for the following variables, in the same directory as this notebook:\n",
    "```\n",
    "OPENAI_API_KEY=<YOUR OPENAI API KEY>\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "LANGCHAIN_PROJECT='langsmith-wikirag-walkthrough'\n",
    "LANGCHAIN_API_KEY=<YOUR LANGSMITH API KEY>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945ce28f-aeae-41ee-88f3-f86af63ae609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Module: null prototype] { default: {} }"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import \"dotenv/config\"; // Load env vars from .env file\n",
    "import OpenAI from \"npm:openai\";\n",
    "import wiki from \"npm:wikipedia\";\n",
    "import { Client } from \"langsmith\";\n",
    "import { traceable } from \"langsmith/traceable\";\n",
    "\n",
    "const openai = new OpenAI();\n",
    "const langsmith = new Client();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4032242-5c56-4698-a744-fd432f77558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Wrap the OAI client in traceable to enable logging to LangSmith\n",
    "const traceCreateCompletion = traceable(\n",
    "  openai.chat.completions.create.bind(openai.chat.completions),\n",
    "  { name: \"OpenAI Chat Completion\", run_type: \"llm\" }\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f5d23-2da7-4d6a-aeaa-cc3812188322",
   "metadata": {},
   "source": [
    "## Create a Wikipedia Rag Pipeline that doesn't use LangChain\n",
    "\n",
    "Here, we'll create a very simple RAG pipeline that:\n",
    "1. Generates a Wikpedia search query from the input question\n",
    "2. Retrieves relevant page summaries from Wikipedia based on the search query\n",
    "3. Answers the input question based on the context from the retrieval step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93190a21-b0ec-45e8-b671-78bd1194f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "async function generateWikiSearch(input: {question: string}): Promise<string> {\n",
    "    const messages = [\n",
    "        { role: \"system\", content: \"Generate a search query to pass into wikipedia to answer the user's question. Return only the search query and nothing more. This will be passed in directly to the Wikipedia search engine.\" },\n",
    "        { role: \"user\", content: input.question }\n",
    "    ];\n",
    "\n",
    "    const chatCompletion = await traceCreateCompletion({\n",
    "        model: \"gpt-3.5-turbo\",\n",
    "        messages: messages,\n",
    "        temperature: 0,\n",
    "    });\n",
    "    return chatCompletion.choices[0].message.content;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afcdfe83-3f57-45db-9d7d-0f5dbc797050",
   "metadata": {},
   "outputs": [],
   "source": [
    "function convertDocs(results: Array<{ summary: string; url: string }>): Array<{ page_content: string; type: string; metadata: { url: string } }> {\n",
    "    // Convert docs to a format that LangSmith accepts (for nicer rendering)\n",
    "    return results.map(r => ({\n",
    "        page_content: r.summary,\n",
    "        type: \"Document\",\n",
    "        metadata: { url: r.url }\n",
    "    }));\n",
    "}\n",
    "\n",
    "async function retrieve(input: {query: string, numDocuments: number}): Promise<Array<{ summary: string; url: string }>> {\n",
    "    const { results } = await wiki.search(input.query, { limit: 10 });\n",
    "    const finalResults: Array<{ summary: string; url: string }> = [];\n",
    "\n",
    "    for (const result of results) {\n",
    "        if (finalResults.length >= input.numDocuments) {\n",
    "            // Just return the top 2 pages for now\n",
    "            break;\n",
    "        }\n",
    "        const page = await wiki.page(result.title, { autoSuggest: false });\n",
    "        const summary = await page.summary();\n",
    "        finalResults.push({\n",
    "            summary: summary.extract,\n",
    "            url: page.fullurl\n",
    "        });\n",
    "    }\n",
    "    \n",
    "    return convertDocs(finalResults);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aa6f777-0149-45df-8974-7db209d10c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "async function generateAnswer(input: {question: string, context: string}): Promise<string> {\n",
    "    const messages = [\n",
    "        { role: \"system\", content: `Answer the user's question based only on the content below:\\n\\n${input.context}` },\n",
    "        { role: \"user\", content: input.question }\n",
    "    ];\n",
    "\n",
    "    const chatCompletion = await traceCreateCompletion({\n",
    "        model: \"gpt-3.5-turbo\",\n",
    "        messages: messages,\n",
    "        temperature: 0,\n",
    "    });\n",
    "    return chatCompletion.choices[0].message.content;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ec7c78-4972-4b94-adcc-b99113114918",
   "metadata": {},
   "outputs": [],
   "source": [
    "const traceGenerateWikiSearch = traceable(generateWikiSearch);\n",
    "const traceRetrieve = traceable(retrieve, { name: \"Retrieve Wiki\", run_type: \"retriever\" });\n",
    "const traceGenerateAnswer = traceable(generateAnswer);\n",
    "\n",
    "const traceRagPipeline = traceable(async ({ question }, numDocuments: number = 2) => {\n",
    "    const query = await traceGenerateWikiSearch({ question });\n",
    "    const retrieverResults = await traceRetrieve({query, numDocuments });\n",
    "    const context = retrieverResults.map(result => result.page_content).join(\"\\n\\n\");\n",
    "    const answer = await traceGenerateAnswer({ question, context });\n",
    "    return answer\n",
    "},\n",
    "{ name: \"Wiki RAG Pipeline\", run_type: \"llm\" });"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5814c348-d746-4efe-97aa-641769895da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"The Apple Vision Pro was released in the United States on February 2, 2024.\"\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await traceRagPipeline({question: \"When was the Apple Vision Pro released in the US?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315b3d7d-ab4f-4267-9190-c18800edbb0d",
   "metadata": {},
   "source": [
    "![Screenshot of Trace View in LangSmith](trace_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf8e87-6b98-4d9f-bc56-8eb9004615d9",
   "metadata": {},
   "source": [
    "## Run the pipeline on some test cases\n",
    "\n",
    "Before deploying to an initial set of users, it's often helpful to create a test set of a few examples, then run your pipeline on the test set.\n",
    "\n",
    "LangSmith makes it easy to run custom evaluations (both LLM and heuristic-based) to score the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1abfef0f-6188-4566-91c9-6ba9c50e0c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ \u001b[90mundefined\u001b[39m, \u001b[90mundefined\u001b[39m, \u001b[90mundefined\u001b[39m ]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a dataset to be used for testing using the LangSmith client\n",
    "const examples = [\n",
    "    [\"When was the Apple Vision Pro released in the US?\", \"The Apple Vision Pro was released in the United States on February 2, 2024.\"],\n",
    "    [\"What is LangChain?\", \"LangChain is an open-source framework for building applications using large language models.\"],\n",
    "    [\"Who is the chairman of OpenAI?\", \"Bret Taylor is the chairman of OpenAI\"]\n",
    "]\n",
    "\n",
    "const datasetName = \"Wikipedia RAG Pipeline\";\n",
    "const dataset = await langsmith.createDataset(datasetName);\n",
    "\n",
    "await Promise.all(examples.map(async ([question, answer]) => {\n",
    "    await langsmith.createExample(\n",
    "        { question },\n",
    "        { answer },\n",
    "        { datasetId: dataset.id },\n",
    "    );\n",
    "}));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d00e485-12e0-48d8-8028-e289db964e81",
   "metadata": {},
   "source": [
    "![Dataset](dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7be9c081-c974-4e35-8fb6-a3cf398fb655",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Run a set of tests on the dataset and compare them in LangSmith\n",
    "// First, set up evaluators to run against the test results\n",
    "import type { DynamicRunEvaluatorParams } from \"langchain/smith\";\n",
    "import { runOnDataset, Criteria, LabeledCriteria } from \"langchain/smith\";\n",
    "// An illustrative custom evaluator example\n",
    "const containsOpenAI = async ({\n",
    "    run,\n",
    "    example,\n",
    "    input,\n",
    "    prediction,\n",
    "    reference,\n",
    "}: DynamicRunEvaluatorParams) => {\n",
    "    return {\n",
    "        key: \"contains_openai\",\n",
    "        score: prediction.output.includes(\"OpenAI\"),\n",
    "    };\n",
    "};\n",
    "\n",
    "const evaluators: RunEvalConfig[\"evaluators\"] = [\n",
    "  // LangChain's built-in evaluators\n",
    "  Criteria(\"conciseness\"),\n",
    "  LabeledCriteria(\"correctness\"),\n",
    "    \n",
    "  // Custom evaluators can be user-defined RunEvaluator's\n",
    "  // or a compatible function\n",
    "  containsOpenAI,\n",
    "];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89feef74-28af-498f-987a-9d5064ebd493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓ 100.00% | 3/3\n",
      "\n",
      "Completed\n",
      "Running Evaluators: ▓▓▓▓▓▓▓▓▓▓▓▓▓░░░░░░░░░░░░░░░░░░░░░░░░░░░ 33.33% | 1/3\n",
      "\n",
      "Running Evaluators: ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓░░░░░░░░░░░░░ 66.67% | 2/3\n",
      "\n",
      "Running Evaluators: ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓ 100.00% | 3/3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Use `runOnDataset` to run the pipeline against examples in the Dataset\n",
    "await runOnDataset(\n",
    "    traceRagPipeline, \n",
    "    datasetName,\n",
    "    evaluators\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0150efa-d17c-4848-8d71-4fbcafb91e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓ 100.00% | 3/3\n",
      "\n",
      "Completed\n",
      "Running Evaluators: ▓▓▓▓▓▓▓▓▓▓▓▓▓░░░░░░░░░░░░░░░░░░░░░░░░░░░ 33.33% | 1/3\n",
      "\n",
      "Running Evaluators: ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓░░░░░░░░░░░░░ 66.67% | 2/3\n",
      "\n",
      "Running Evaluators: ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓ 100.00% | 3/3\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  projectName: \u001b[32m\"terrific-rub-87\"\u001b[39m,\n",
       "  results: {\n",
       "    \u001b[32m\"60ec0fe3-f790-4475-ba50-9446327d11da\"\u001b[39m: {\n",
       "      execution_time: \u001b[33m6493\u001b[39m,\n",
       "      feedback: [\n",
       "        {\n",
       "          id: \u001b[32m\"534e61f4-781b-4204-a792-e1ecf1081651\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"ea973726-bf2c-44a6-b1aa-efc7de7dba09\"\u001b[39m,\n",
       "          key: \u001b[32m\"contains_openai\"\u001b[39m,\n",
       "          score: \u001b[33mfalse\u001b[39m,\n",
       "          value: \u001b[90mundefined\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[90mundefined\u001b[39m,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"5c5e0c68-3656-4c31-b648-b58e31e9ec37\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"ea973726-bf2c-44a6-b1aa-efc7de7dba09\"\u001b[39m,\n",
       "          key: \u001b[32m\"conciseness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is conciseness. This means the submission should be brief, to the point,\"\u001b[39m... 595 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"7fe4e967-240b-448a-89d1-6a5541cf277e\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"ea973726-bf2c-44a6-b1aa-efc7de7dba09\"\u001b[39m,\n",
       "          key: \u001b[32m\"correctness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is the correctness of the submission. The submission states that LangCha\"\u001b[39m... 555 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        }\n",
       "      ],\n",
       "      run_id: \u001b[32m\"ea973726-bf2c-44a6-b1aa-efc7de7dba09\"\u001b[39m\n",
       "    },\n",
       "    \u001b[32m\"c89d2a91-fcfb-489e-914f-1273c6a96e0c\"\u001b[39m: {\n",
       "      execution_time: \u001b[33m3896\u001b[39m,\n",
       "      feedback: [\n",
       "        {\n",
       "          id: \u001b[32m\"cfd9d482-a528-45c4-9a08-2e35ef10d6dc\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"ad78ef69-58f7-4578-8999-c51ff02456ab\"\u001b[39m,\n",
       "          key: \u001b[32m\"contains_openai\"\u001b[39m,\n",
       "          score: \u001b[33mtrue\u001b[39m,\n",
       "          value: \u001b[90mundefined\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[90mundefined\u001b[39m,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"30ed4617-9e4c-41a8-846c-672d486248b0\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"ad78ef69-58f7-4578-8999-c51ff02456ab\"\u001b[39m,\n",
       "          key: \u001b[32m\"conciseness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is conciseness. This means the submission should be brief, to the point,\"\u001b[39m... 389 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"31f8dae8-5d16-4c2e-8053-d53c66cb5681\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"ad78ef69-58f7-4578-8999-c51ff02456ab\"\u001b[39m,\n",
       "          key: \u001b[32m\"correctness\"\u001b[39m,\n",
       "          score: \u001b[33m0\u001b[39m,\n",
       "          value: \u001b[32m\"N\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is the correctness of the submission. The submission should be correct, \"\u001b[39m... 437 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        }\n",
       "      ],\n",
       "      run_id: \u001b[32m\"ad78ef69-58f7-4578-8999-c51ff02456ab\"\u001b[39m\n",
       "    },\n",
       "    \u001b[32m\"6e0dec3d-6a05-4c04-933b-d7c41d8c8097\"\u001b[39m: {\n",
       "      execution_time: \u001b[33m4483\u001b[39m,\n",
       "      feedback: [\n",
       "        {\n",
       "          id: \u001b[32m\"d6b21c0d-99f2-43e0-b4c6-379041e7fc2a\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"d389b496-4ef2-4dd0-8890-e1c9612d830c\"\u001b[39m,\n",
       "          key: \u001b[32m\"contains_openai\"\u001b[39m,\n",
       "          score: \u001b[33mfalse\u001b[39m,\n",
       "          value: \u001b[90mundefined\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[90mundefined\u001b[39m,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"09399aaf-ddb1-4f9c-a58f-315625fb0bba\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"d389b496-4ef2-4dd0-8890-e1c9612d830c\"\u001b[39m,\n",
       "          key: \u001b[32m\"conciseness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is conciseness. This means the submission should be brief, to the point,\"\u001b[39m... 309 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        },\n",
       "        {\n",
       "          id: \u001b[32m\"35ff3a89-c221-4256-9a82-942a7ca82397\"\u001b[39m,\n",
       "          run_id: \u001b[32m\"d389b496-4ef2-4dd0-8890-e1c9612d830c\"\u001b[39m,\n",
       "          key: \u001b[32m\"correctness\"\u001b[39m,\n",
       "          score: \u001b[33m1\u001b[39m,\n",
       "          value: \u001b[32m\"Y\"\u001b[39m,\n",
       "          correction: \u001b[90mundefined\u001b[39m,\n",
       "          comment: \u001b[32m\"The criterion for this task is the correctness of the submission. This involves checking if the subm\"\u001b[39m... 391 more characters,\n",
       "          feedback_source: \u001b[36m[Object]\u001b[39m\n",
       "        }\n",
       "      ],\n",
       "      run_id: \u001b[32m\"d389b496-4ef2-4dd0-8890-e1c9612d830c\"\u001b[39m\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Let's now execute a test of the pipeline where we retrieve 4 pages instead of 1\n",
    "\n",
    "// Wrapper function with numDocuments set to 4\n",
    "const traceRagPipelineFourDocuments = async ({ question }) => {\n",
    "    return traceRagPipeline({ question }, 4);\n",
    "};\n",
    "\n",
    "\n",
    "await runOnDataset(\n",
    "    traceRagPipelineFourDocuments,\n",
    "    datasetName,\n",
    "    evaluators\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe97c517-63a7-4312-be61-c68db06a58fe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "![Screenshot of Test Results in LangSmith](test_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a9d36-f75d-4799-b132-9b36127ae03e",
   "metadata": {},
   "source": [
    "![Screenshot of Comparison View in LangSmith](comparison_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408b3934-c865-4f8c-b686-8896f7d25877",
   "metadata": {},
   "source": [
    "## Tracing\n",
    "\n",
    "Let's say you've deployed your application to production or to an initial set of users. \n",
    "\n",
    "You can view traces of your application in LangSmith and drill down by various attributes to get statistics on a specific set of traces.\n",
    "\n",
    "You can also attach feedback to your runs (such as through a thumbs up/down button) [using the LangSmith client](https://docs.smith.langchain.com/tracing/faq/logging_feedback#collecting-feedback-programmatically), and filter on traces with a certain feedback score in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1baedc0f-0578-42d7-be79-30890e7e2d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  \u001b[32m\"Sam Altman was removed from OpenAI on November 17, 2023.\"\u001b[39m,\n",
       "  \u001b[32m\"The significance of the James Webb Space Telescope's first images, specifically the Webb's First Dee\"\u001b[39m... 395 more characters,\n",
       "  \u001b[32m\"The international community responded to the crisis in Ukraine by imposing sanctions on Russia and C\"\u001b[39m... 450 more characters,\n",
       "  \u001b[32m\"As of 3 January 2024, 13.53 billion COVID-19 vaccine doses have been administered worldwide. Approxi\"\u001b[39m... 381 more characters,\n",
       "  \u001b[32m\"The content provided does not mention any specific outcomes of the recent G7 summit.\"\u001b[39m,\n",
       "  \u001b[32m\"The content provided does not mention any specific leading figures in climate change activism.\"\u001b[39m,\n",
       "  \u001b[32m\"Vanta can refer to multiple things. It can be a city in Finland called Vantaa, a graphics accelerato\"\u001b[39m... 398 more characters,\n",
       "  \u001b[32m\"Jannik Sinner has won one major, specifically the Australian Open in 2024.\"\u001b[39m\n",
       "]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Here we execute the pipeline on a number of potential user inputs, simulating how users might interact with it.\n",
    "import { pLimit } from \"https://deno.land/x/p_limit@v1.0.0/mod.ts\";\n",
    "const limit = pLimit(5); // Add a concurrency limit to avoid getting rate limited\n",
    "\n",
    "async function runRagPipelines(questions: string[]): Promise<string[]> {\n",
    "    const promises = questions.map(question =>\n",
    "        limit(() => traceRagPipeline({ question }))\n",
    "    );\n",
    "\n",
    "    try {\n",
    "        const results = await Promise.all(promises);\n",
    "        return results;\n",
    "    } catch (error) {\n",
    "        console.error(\"Error running RAG Pipelines:\", error);\n",
    "        throw error;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Example usage\n",
    "const questions: string[] = [\n",
    "    \"When was Sam Altman removed from OpenAI?\",\n",
    "    \"What is the significance of the James Webb Space Telescope's first images?\",\n",
    "    \"How did the international community respond to the crisis in Ukraine?\",\n",
    "    \"What is the status of the COVID-19 vaccine distribution worldwide?\",\n",
    "    \"What are the outcomes of the recent G7 summit?\",\n",
    "    \"Who are the leading figures in climate change activism today?\",\n",
    "    \"What is Vanta?\",\n",
    "    \"How many majors has Jannik Sinner won?\"\n",
    "];\n",
    "await runRagPipelines(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4361279-6075-4225-a918-50bae60b04e8",
   "metadata": {},
   "source": [
    "![Screenshot of Trace View in LangSmith](trace_project_view.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6aa739-c7d9-4e2c-90a7-237088353fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
